% Existing papers from literature review
@article{prakash2025,
  title={Multi-ASR Fusion with LLM-Based Post-Editing for High-Quality Pseudo-Labels},
  author={Prakash, A. and others},
  journal={Proceedings of Interspeech},
  year={2025},
  note={Demonstrates LLM-based multi-ASR fusion with 14\% WERR on LibriSpeech}
}

@article{nagarathna2025,
  title={TruCLeS: True Class Lexical Similarity Score for Novel Confidence Estimation in ASR},
  author={Nagarathna, C. and others},
  journal={IEEE Transactions on Audio, Speech, and Language Processing},
  year={2025},
  note={Novel confidence metric combining model probabilities with lexical similarity}
}

@article{koilakuntla2024,
  title={LLM-Based ASR Error Correction for Contact Center Domain-Specific Terminology},
  author={Koilakuntla, M. and others},
  journal={Proceedings of ICASSP},
  year={2024},
  note={GPT-3.5 based correction with retrieval-augmented generation for domain-specific errors}
}

@inproceedings{parikh2024,
  title={Hybrid-E2E ASR Ensemble with Confidence Calibration for Low-Resource Irish Language},
  author={Parikh, N. and others},
  booktitle={Proceedings of LREC-COLING},
  year={2024},
  note={ROVER fusion with calibrated confidences achieves 14\% relative improvement}
}

@article{naqvi2024,
  title={Code-Mixed Urdu-English ASR for Navigation Domain Using Hybrid Architecture},
  author={Naqvi, S. A. and Tahir, M. A.},
  journal={Journal of Natural Language Processing},
  year={2024},
  note={Specialized Kaldi-based system achieving 4.02\% WER on street addresses}
}

% New papers to be added
@article{hori2025,
  title={Delayed Fusion: Integrating Large Language Models into First-Pass Decoding in End-to-end Speech Recognition},
  author={Hori, Takaaki and others},
  journal={arXiv preprint arXiv:2501.09258},
  year={2025},
  url={https://arxiv.org/abs/2501.09258},
  note={Proposes delayed fusion method for efficient LLM integration in ASR decoding}
}

@article{ma2024,
  title={ASR Error Correction using Large Language Models},
  author={Ma, Rao and Qian, Mengjie and Gales, Mark and Knill, Kate},
  journal={arXiv preprint arXiv:2409.09554},
  year={2024},
  url={https://arxiv.org/abs/2409.09554},
  note={Investigates N-best list correction with LLMs including zero-shot approaches}
}

@inproceedings{feng2024,
  title={Low-resource Language Adaptation with Ensemble of PEFT Approaches},
  author={Feng, Wei and others},
  booktitle={Proceedings of the 2024 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)},
  year={2024},
  address={Macau, China},
  url={http://www.apsipa2024.org/files/papers/37.pdf},
  note={Ensemble of diverse PEFT methods reduces WER from 8.4\% to 7.9\% with 5x less memory}
}

@article{arif2024,
  title={WER We Stand: Benchmarking Urdu ASR Models},
  author={Arif, Samee and others},
  journal={arXiv preprint},
  year={2024},
  note={First comprehensive Urdu ASR benchmark with conversational test set and fine-tuned SOTA models}
}

@inproceedings{gitman2023,
  title={Confidence-based Ensembles of End-to-End Speech Recognition Models},
  author={Gitman, Igor and others},
  booktitle={Proceedings of Interspeech},
  year={2023},
  note={Confidence-driven model selection achieves near-oracle ensemble performance without joint training}
}