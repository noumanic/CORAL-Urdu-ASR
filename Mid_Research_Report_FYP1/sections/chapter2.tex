% sections/chapter2.tex
\chapter{Literature Review}
\label{sec:literature}

This chapter critically examines existing literature relevant to the CORAL project. We analyze key research in multi-ASR fusion, confidence estimation, LLM-based error correction, and low-resource language ASR systems, establishing the theoretical and empirical foundation for our approach.

\section{Related Research}

We review ten seminal papers that directly inform the CORAL architecture's design and methodology.

\subsection{Multi-ASR Fusion with LLM-Based Post-Editing}

\subsubsection{Summary: Prakash et al. (2025)}

Prakash et al. \cite{prakash2025} introduce a novel approach for generating high-quality pseudo-labels for semi-supervised ASR training by unifying outputs from multiple end-to-end ASR models using LLM-based post-editing. Their method integrates three diverse ASR systems: Icefall, Nemo Parakeet, and Whisper, leveraging their complementary strengths.

The key innovation is their two-stage approach:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Text-Based LLM Post-Editing}: An instruction-tuned LLM processes textual hypotheses from all three ASR systems, using natural language prompts to correct errors and synthesize a final transcript.
    
    \item \textbf{SpeechLLM with Audio Input}: A multimodal LLM that processes both textual hypotheses and the original audio signal, enabling audio-informed correction decisions.
\end{enumerate}

On LibriSpeech datasets, their approach achieved approximately 14\% relative Word Error Rate Reduction (WERR) after fusion. When used to generate pseudo-labels for semi-supervised training, ASR models retrained on these pseudo-labels achieved 3.22\% WER on LibriSpeech test-clean versus 3.40\% for the baseline, approaching near-human-level performance.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Demonstrates the effectiveness of LLM-based multi-ASR fusion
    \item Achieves significant relative error reduction (14\% WERR)
    \item Novel integration of audio signals into LLM correction process
    \item Validated on widely-recognized benchmark datasets
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Requires expensive LLM fine-tuning, limiting accessibility
    \item High computational requirements (GPU-intensive)
    \item Tested exclusively on high-resource English datasets
    \item No evaluation on low-resource languages or code-switched speech
    \item Complexity of multimodal SpeechLLM limits practical deployment
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL builds directly upon this work but addresses its limitations for low-resource scenarios:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{No Fine-Tuning Required}: CORAL uses black-box instruction-tuned LLMs with zero-shot prompting, eliminating fine-tuning costs
    \item \textbf{Confidence-Guided Fusion}: Instead of only textual hypotheses, CORAL provides explicit word-level confidence scores to guide LLM decisions
    \item \textbf{Low-Resource Focus}: Specifically designed and evaluated for Urdu, a genuinely low-resource language
    \item \textbf{Text-Only Simplicity}: Maintains practicality by not requiring audio input to LLM, reducing complexity and computational costs
\end{itemize}

\subsection{Novel Confidence Estimation for ASR}

\subsubsection{Summary: Nagarathna et al. (2025)}

Nagarathna et al. \cite{nagarathna2025} propose TruCLeS (True Class Lexical Similarity Score), a novel continuous confidence metric for ASR systems that combines model probability scores with lexical similarity measures.

Traditional confidence metrics rely solely on model output probabilities, which often suffer from miscalibration. TruCLeS addresses this by:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item Computing ASR model probability scores for predicted tokens
    \item Calculating lexical similarity between predictions and potential ground-truth transcripts
    \item Combining both signals into a unified continuous confidence score
\end{enumerate}

The method requires ground-truth transcripts for training a supervised confidence model but demonstrates superior calibration performance across multiple metrics (Mean Absolute Error, Kullback-Leibler Divergence, Jensen-Shannon Divergence) compared to binary confidence baselines.

On in-domain Hindi data, TruCLeS reduced MAE from 0.108 to 0.087, showing consistent improvements in confidence calibration quality.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Novel approach combining probability and lexical similarity
    \item Demonstrated improvement in calibration metrics
    \item Applicable to low-resource languages (tested on Hindi)
    \item Addresses fundamental miscalibration issues in neural ASR
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Requires ground-truth transcripts for training, limiting scalability
    \item Adds computational overhead of auxiliary confidence model
    \item Improves calibration but does not directly reduce WER
    \item Complexity may limit real-time deployment scenarios
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL takes a complementary approach to confidence estimation:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{Zero-Shot Confidence}: CORAL extracts raw model confidences without requiring additional training or ground-truth data
    \item \textbf{Ensemble Diversity}: Instead of improving individual model calibration, CORAL leverages confidence differences across multiple models
    \item \textbf{Practical Deployment}: Avoids auxiliary models, maintaining computational efficiency
    \item \textbf{Direct WER Impact}: Uses confidence scores for hypothesis selection and fusion, directly impacting final transcription accuracy
\end{itemize}

While TruCLeS focuses on improving confidence quality, CORAL focuses on leveraging diverse confidence signals for hypothesis correction.

\subsection{Domain-Specific LLM-Based ASR Correction}

\subsubsection{Summary: Koilakuntla et al. (2024)}

Koilakuntla et al. \cite{koilakuntla2024} present a targeted approach for correcting specific ASR errors in contact center environments using GPT-3.5 with retrieval-augmented generation. Their method addresses the challenge of domain-specific terminology (brand names, product codes, technical terms) that generic ASR systems frequently misrecognize.

The approach uses:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Error Pattern Identification}: Analyzes transcripts to identify specific error types (e.g., brand name misrecognitions)
    \item \textbf{Retrieval-Augmented Correction}: Uses context anchors and domain knowledge bases to provide GPT-3.5 with relevant correction candidates
    \item \textbf{Targeted Post-Processing}: Applies corrections selectively to identified error patterns rather than reprocessing entire transcripts
\end{enumerate}

The system corrected 3,201 instances of specific errors compared to 3,050 manual corrections, completing the task in 0.08 hours versus 15 hours manually - a dramatic efficiency improvement.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Model-agnostic: Works with any external ASR provider
    \item Highly effective for targeted error correction
    \item Dramatic reduction in manual correction time (99.5\% reduction)
    \item Practical deployment in production environments
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Highly specialized for specific domains (contact centers)
    \item Limited generalizability to other error types or domains
    \item Requires careful prompt engineering for each error category
    \item Needs curated retrieval databases for domain-specific terms
    \item No overall WER improvement reported - only targeted correction metrics
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL differs fundamentally in scope and approach:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{General-Purpose Correction}: CORAL aims for overall WER reduction across all error types, not just specific patterns
    \item \textbf{Open-Domain Application}: Designed for diverse Urdu speech domains without requiring domain-specific engineering
    \item \textbf{Confidence-Driven}: Uses confidence scores from ensemble to guide corrections, rather than pattern matching
    \item \textbf{No Retrieval Required}: Operates without external knowledge bases, leveraging LLM's intrinsic linguistic knowledge
\end{itemize}

However, CORAL can learn from their success in structured prompt design for LLM-based correction.

\subsection{Hybrid-E2E ASR Ensemble for Low-Resource Languages}

\subsubsection{Summary: Parikh et al. (2024)}

Parikh et al. \cite{parikh2024} address ASR for Irish, a genuinely low-resource language, by combining complementary strengths of hybrid HMM-Kaldi systems and end-to-end Wav2Vec2.0 models through calibrated ROVER (Recognizer Output Voting Error Reduction) fusion.

Their approach involves:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Diverse System Architectures}: Combining traditional hybrid HMM-DNN (Kaldi) with modern self-supervised E2E (Wav2Vec2.0 XLS-R)
    \item \textbf{Confidence Calibration}: Addressing overconfidence issues in E2E models using Renyi's entropy-based calibration with temperature scaling
    \item \textbf{ROVER Fusion}: Word-level weighted voting based on calibrated confidence scores
\end{enumerate}

On Irish test data, their tuned ROVER ensemble achieved 22.94\% WER, representing a 14\% relative improvement over the best single model (25.81\% WER). The work demonstrates that even simple fusion techniques can yield significant gains for low-resource scenarios.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Demonstrates effectiveness for genuinely low-resource language (Irish)
    \item Achieves 14-20\% relative WER reduction through ensemble
    \item Addresses E2E model overconfidence through principled calibration
    \item Combines traditional and modern ASR approaches effectively
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Uses traditional ROVER fusion without modern LLM capabilities
    \item Requires building and maintaining two distinct ASR systems (hybrid + E2E)
    \item Absolute WER remains relatively high (22.94\%)
    \item Confidence calibration requires tuning on development set
    \item No mechanism for linguistic coherence beyond voting
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL extends this ensemble concept with modern techniques:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{LLM-Based Fusion}: Replaces ROVER voting with instruction-guided LLM reasoning for linguistically coherent correction
    \item \textbf{Zero-Shot Calibration}: Avoids explicit calibration tuning by leveraging multiple models' raw confidences
    \item \textbf{Multiple E2E Models}: Uses diverse pre-trained E2E models (Whisper variants, Wav2Vec2) without requiring hybrid systems
    \item \textbf{Similar Target}: Like Irish, Urdu is a low-resource language, making this work highly relevant
\end{itemize}

CORAL aims to achieve similar or better relative gains with lower system complexity.

\subsection{Code-Mixed ASR for Urdu-English}

\subsubsection{Summary: Naqvi \& Tahir (2024)}

Naqvi and Tahir \cite{naqvi2024} develop a specialized hybrid ASR system for Urdu-English code-mixed street addresses in navigation contexts. They address the specific challenge of code-switching between Urdu (in Perso-Arabic script) and English (in Roman script) within the narrow domain of spoken addresses.

Their approach involves:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Specialized Corpus Collection}: 61.8 hours of general Urdu speech and 16.9 hours of Roman-Urdu/English addresses
    \item \textbf{Hybrid Architecture}: Kaldi-based system with TDNN-LSTM acoustic models
    \item \textbf{Custom Lexicon}: Combining Unicode Urdu and Romanized transcripts for code-mixed handling
    \item \textbf{Domain Optimization}: Deep specialization for navigation/address domain
\end{enumerate}

The system achieved remarkably low WER (4.02\%) and CER (0.8\%) on code-mixed street addresses, representing a 70-80\% absolute WER reduction compared to initial baselines.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Extremely low error rates in target domain (4.02\% WER)
    \item Explicitly handles Urdu-English code-switching
    \item Practical deployment for navigation applications
    \item Demonstrates feasibility of Urdu ASR with sufficient domain data
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Very narrow scope: limited to street addresses and navigation
    \item Not an ensemble or LLM-based approach
    \item Requires extensive domain-specific data collection and engineering
    \item Single hybrid system without multi-model benefits
    \item Unlikely to generalize to other Urdu domains or open-domain speech
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL complements this work by targeting broader applicability:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{Open-Domain Focus}: CORAL aims for general Urdu ASR without domain restrictions
    \item \textbf{Code-Switching Capability}: Multilingual pre-trained models (Whisper, Wav2Vec2-XLSR) inherently handle code-switching
    \item \textbf{No Custom Data Required}: Leverages pre-trained models without domain-specific corpus collection
    \item \textbf{Ensemble Benefits}: Multiple models provide robustness across various code-switching patterns
\end{itemize}

While Naqvi \& Tahir achieve superior performance in their narrow domain, CORAL pursues broader applicability with acceptable performance tradeoffs.

\subsection{Delayed Fusion for LLM Integration in ASR}

\subsubsection{Summary: Hori et al. (2025)}

Hori et al. \cite{hori2025} propose "delayed fusion," a novel method for integrating large language models into first-pass ASR decoding. Their approach addresses two critical challenges: (1) the computational cost of LLM inference, and (2) vocabulary mismatches between ASR and LLM models.

The key innovation involves:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Delayed Score Application}: LLM scores are applied to ASR hypotheses with a delay during beam search decoding
    \item \textbf{Reduced LLM Calls}: Significantly fewer hypotheses need to be scored by the LLM
    \item \textbf{No Retraining Required}: Pre-trained LLMs can be integrated without vocabulary alignment or model modification
\end{enumerate}

Delayed fusion reduces both the number of hypotheses scored and the total number of LLM inference calls, making it more efficient than traditional shallow fusion or N-best rescoring approaches.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Computationally efficient LLM integration
    \item No vocabulary alignment or model retraining needed
    \item Reduces LLM inference overhead significantly
    \item Applicable to any pre-trained LLM
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Requires modification of ASR decoding algorithm
    \item Limited evaluation on low-resource languages
    \item May not capture full LLM reasoning capabilities
    \item Timing of delay parameter requires tuning
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL takes a different approach but shares the efficiency goal:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{Post-Processing vs. First-Pass}: CORAL uses LLM for post-correction rather than during decoding, simplifying integration
    \item \textbf{Black-Box Models}: Both approaches work with pre-trained models without modification
    \item \textbf{Efficiency Consideration}: CORAL's confidence-guided approach similarly aims to optimize LLM usage
    \item \textbf{Complementary Methods}: Delayed fusion could potentially be combined with CORAL's post-correction stage
\end{itemize}

\subsection{N-Best List Error Correction with LLMs}

\subsubsection{Summary: Ma et al. (2024)}

Ma et al. \cite{ma2024} investigate using large language models for ASR error correction across diverse scenarios, with emphasis on N-best list utilization. Their work demonstrates that using multiple competing hypotheses (N-best lists) provides richer contextual information for LLM-based correction.

Key contributions include:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{N-Best List Correction}: Using multiple ASR hypotheses rather than single best output
    \item \textbf{Zero-Shot Correction}: Evaluating ChatGPT and similar LLMs without task-specific training
    \item \textbf{Cross-System Generalization}: Testing correction models trained on one ASR system's output on different systems
    \item \textbf{Ensemble Alternative}: N-best correction serves as implicit model ensembling
\end{enumerate}

The research shows that N-best lists enable more effective error correction than single-hypothesis approaches, and zero-shot LLMs can perform competitive correction without supervised training.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Demonstrates value of multiple hypotheses for correction
    \item Zero-shot approaches reduce training requirements
    \item Cross-system evaluation validates generalization
    \item Practical for deployment with existing ASR systems
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item N-best lists may not capture diverse model perspectives
    \item Limited evaluation on low-resource languages
    \item No explicit confidence score utilization
    \item Computational cost of processing N-best lists
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL extends the N-best concept to multi-model ensembles:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{Multi-Model vs. Single-Model N-Best}: CORAL uses hypotheses from diverse models rather than N-best from one model
    \item \textbf{Explicit Confidence}: CORAL provides word-level confidence scores, not just ranked hypotheses
    \item \textbf{Model Diversity}: Different architectures provide more varied perspectives than N-best from same model
    \item \textbf{Zero-Shot LLM}: Both approaches leverage instruction-tuned LLMs without task-specific training
\end{itemize}

\subsection{Parameter-Efficient Ensemble for Low-Resource Languages}

\subsubsection{Summary: Feng et al. (2024)}

Feng et al. \cite{feng2024} demonstrate that ensembles of models adapted using diverse Parameter-Efficient Fine-Tuning (PEFT) methods consistently outperform ensembles using the same PEFT method. Their approach achieves significant WER reduction (8.4\% to 7.9\%) while requiring approximately five times less memory than traditional fully fine-tuned ensemble approaches.

Key innovations include:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Diverse PEFT Methods}: Combining LoRA, adapters, prefix tuning, and other PEFT techniques
    \item \textbf{ROVER-Based Fusion}: Using confidence-weighted voting to aggregate outputs
    \item \textbf{Memory Efficiency}: Dramatically reduced memory requirements compared to full fine-tuning
    \item \textbf{Low-Resource Focus}: Validated on multiple low-resource language adaptation tasks
\end{enumerate}

The diverse PEFT ensemble approach demonstrates that architectural diversity in adaptation methods provides complementary benefits similar to model architecture diversity.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Memory-efficient alternative to traditional ensembles
    \item Demonstrates value of diverse adaptation methods
    \item Practical for resource-constrained scenarios
    \item Validated on multiple low-resource languages
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Still requires some fine-tuning (even if parameter-efficient)
    \item ROVER fusion limited compared to modern LLM approaches
    \item Complexity of maintaining multiple PEFT variants
    \item Performance gains modest compared to full ensemble
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL takes a complementary no-tuning approach:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{Zero Fine-Tuning}: CORAL uses pre-trained models without any adaptation
    \item \textbf{LLM vs. ROVER}: CORAL employs LLM reasoning rather than voting
    \item \textbf{Similar Philosophy}: Both leverage diversity for improved performance
    \item \textbf{Resource Efficiency}: Both prioritize practical deployment constraints
\end{itemize}

For future work, CORAL could incorporate PEFT-based adaptation as an optional enhancement stage.

\subsection{Comprehensive Urdu ASR Benchmarking}

\subsubsection{Summary: Arif et al. (2024)}

Arif et al. \cite{arif2024} provide the first comprehensive evaluation of state-of-the-art multilingual ASR models on Urdu. They compile both read and conversational Urdu speech data, introduce the first conversational Urdu ASR test set, and fine-tune multiple ASR families including OpenAI's Whisper, Meta's MMS, and Seamless-M4T.

Key contributions include:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Benchmark Dataset}: First public Urdu conversational ASR test set
    \item \textbf{Model Comparison}: Systematic evaluation of Whisper, MMS, and Seamless models
    \item \textbf{Error Analysis}: Detailed breakdown of insertions, deletions, and substitutions
    \item \textbf{Open Resources}: All models, datasets, and evaluation scripts publicly released
\end{enumerate}

Results show Whisper-large achieves best WER on conversational speech while Seamless-large excels on read speech, with 10-20\% relative WER reduction through fine-tuning. The authors note that integrating large LMs for post-correction could further improve outputs.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Establishes first comprehensive Urdu ASR benchmark
    \item Systematic comparison of multiple SOTA models
    \item Open-source resources promote reproducibility
    \item Detailed error analysis and normalization strategies
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Limited to individual model evaluation (no ensembles)
    \item No confidence score analysis
    \item Code-switching challenges identified but not fully addressed
    \item Fine-tuning required for best results
\end{itemize}

\subsubsection{Relationship to CORAL}

This work provides the foundation for CORAL:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{Baseline Models}: CORAL uses the same pre-trained models (Whisper variants) evaluated here
    \item \textbf{Benchmark Data}: Can leverage their datasets for evaluation
    \item \textbf{Error Types}: Their analysis informs CORAL's correction strategy
    \item \textbf{LLM Integration}: Authors' suggestion of LLM post-correction directly motivates CORAL
\end{itemize}

CORAL builds on their benchmarking work by implementing the multi-model LLM-based correction they propose.

\subsection{Confidence-Based Model Selection for ASR}

\subsubsection{Summary: Gitman et al. (2023)}

Gitman et al. \cite{gitman2023} propose confidence-driven ensembling where multiple expert ASR models run in parallel and the transcript from the most confident model is selected per utterance. Using 5 monolingual Conformer-RNNT models, they show this simple selection scheme outperforms systems using separate language identification blocks.

Key innovations include:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Confidence-Based Selection}: Simple model selection via confidence features
    \item \textbf{No Joint Training}: Black-box expert models without retraining
    \item \textbf{Domain Adaptation}: Combining base and adapted models preserves original performance
    \item \textbf{Near-Oracle Performance}: Achieves results close to oracle ensemble on multilingual benchmarks
\end{enumerate}

Evaluated on VoxPopuli, MLS, Common Voice, and CORAAL datasets, the approach demonstrates consistent WER improvements across languages and accents.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Simple, practical confidence-based selection
    \item No retraining of ASR models required
    \item Effective across multiple domains and languages
    \item Open implementation in NVIDIA NeMo
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Selects single model rather than fusing multiple perspectives
    \item Limited to utterance-level selection (not word-level)
    \item Requires training selection classifier (though lightweight)
    \item No mechanism for linguistic reasoning
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL extends confidence-based selection to word-level fusion:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{Word-Level vs. Utterance-Level}: CORAL uses confidence at word granularity
    \item \textbf{Fusion vs. Selection}: CORAL synthesizes from multiple models rather than selecting one
    \item \textbf{LLM Reasoning}: Adds linguistic coherence beyond confidence scores alone
    \item \textbf{Similar Philosophy}: Both leverage confidence for ensemble coordination
\end{itemize}

Gitman et al.'s work validates that confidence-based approaches are effective and practical for production deployment.

\section{Analysis Summary}

Table \ref{tab:literature_summary_extended} provides a comprehensive comparison of all reviewed research papers across key dimensions relevant to CORAL.

\begin{table}[htbp]
\centering
\caption{Comparative Analysis of Related Research}
\label{tab:literature_summary_extended}
\footnotesize
\setlength{\tabcolsep}{3.5pt}
\renewcommand{\arraystretch}{1.35}
\begin{tabularx}{\textwidth}{|p{2cm}|p{2.8cm}|p{2.8cm}|p{2.8cm}|p{3.3cm}|@{}}
\hline
\textbf{Study} & \textbf{Key Strengths} & \textbf{Limitations} & \textbf{Best Results} & \textbf{CORAL's Innovation} \\ 
\hline
Prakash et al. (2025) & 
Multi-ASR fusion with LLM; Audio-aware SpeechLLM; 14\% WERR & 
Requires LLM fine-tuning; High computational cost; English-only & 
3.22\% WER on LibriSpeech test-clean & 
Black-box LLM; Confidence-guided selection; Urdu-focused \\ 
\hline

Nagarathna et al. (2025) & 
Novel confidence metric (TruCLeS); Superior calibration & 
Requires ground truth for training; No direct WER improvement & 
MAE reduced from 0.108 to 0.087 & 
Zero-shot confidence estimation; Ensemble diversity; Direct WER impact \\ 
\hline

Koilakuntla et al. (2024) & 
Domain-specific correction; 99.5\% time reduction & 
Narrow application scope; Requires custom prompts & 
3,201 corrections in 0.08 hours & 
General-purpose framework; Open-domain; No retrieval needed \\ 
\hline

Parikh et al. (2024) & 
Low-resource language (Irish); 14\% relative improvement; Hybrid+E2E & 
Traditional ROVER fusion; Requires calibration tuning & 
22.94\% WER on Irish dataset & 
LLM-based fusion; Zero-shot calibration; E2E-only systems \\ 
\hline

Naqvi \& Tahir (2024) & 
Code-mixed Urdu-English; 4.02\% WER in-domain & 
Very narrow scope (addresses only); Single system & 
4.02\% WER, 0.8\% CER & 
Open-domain application; Multi-model ensemble; No custom data \\ 
\hline

Hori et al. (2025) & 
Efficient LLM integration; Delayed fusion strategy; No retraining & 
Requires decoding modifications; Limited low-resource evaluation & 
Significantly reduced LLM inference calls & 
Post-processing approach; Simpler integration pipeline \\ 
\hline

Ma et al. (2024) & 
N-best hypothesis correction; Zero-shot LLM; Cross-system generalization & 
Single-model N-best lists; No explicit confidence modeling & 
Competitive zero-shot correction performance & 
Multi-model diversity; Word-level confidence weighting \\ 
\hline

Feng et al. (2024) & 
Memory-efficient PEFT ensemble; 8.4\% to 7.9\% WER; 5$\times$ less memory & 
Still requires fine-tuning; Traditional ROVER fusion & 
7.9\% WER with diverse PEFT configurations & 
Zero fine-tuning required; Advanced LLM reasoning \\ 
\hline

Arif et al. (2024) & 
First comprehensive Urdu ASR benchmark; Conversational test set; Open resources & 
No ensemble evaluation; No confidence analysis & 
10--20\% relative improvement with fine-tuning & 
Multi-model ensemble; Confidence-guided error correction \\ 
\hline

Gitman et al. (2023) & 
Confidence-based selection; Near-oracle performance; No joint training & 
Utterance-level selection only; No linguistic reasoning & 
Near-oracle WER on multilingual benchmarks & 
Word-level fusion; LLM reasoning for linguistic coherence \\ 
\hline
\end{tabularx}
\end{table}

\subsection{Research Gaps Addressed by CORAL}

Based on the literature analysis, we identify critical gaps that CORAL addresses:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Multi-ASR Fusion for Low-Resource Languages}: While Prakash et al. demonstrate LLM-based fusion effectiveness, their work is limited to high-resource English. CORAL extends this approach specifically to low-resource Urdu without requiring expensive fine-tuning.
    
    \item \textbf{Practical Confidence Utilization}: Nagarathna et al. improve confidence calibration but require supervised training. CORAL leverages raw confidence scores in a zero-shot manner, making them immediately actionable for hypothesis correction.
    
    \item \textbf{General-Purpose Open-Domain Correction}: Koilakuntla et al. excel in narrow domains with specific error patterns. CORAL provides general-purpose correction across all error types without domain-specific engineering.
    
    \item \textbf{Modern Fusion Beyond ROVER}: Parikh et al. and Feng et al. demonstrates the ensemble benefits for low-resource languages but use traditional ROVER voting. CORAL employs instruction-tuned LLMs for linguistically coherent fusion that considers context beyond simple voting.
    
    \item \textbf{Scalable Code-Mixing Handling}: Naqvi \& Tahir achieve excellent results in navigation domain but require extensive domain-specific data collection. CORAL leverages multilingual pre-trained models' inherent code-switching capabilities for broader applicability.
    
    \item \textbf{Efficient LLM Integration}: Hori et al. propose delayed fusion during decoding, but this requires modifying ASR algorithms. CORAL's post-processing approach simplifies integration while maintaining efficiency through confidence-guided selection.
    
    \item \textbf{Multi-Model Diversity}: Ma et al. use N-best lists from single models, while CORAL uses hypotheses from diverse model architectures, providing richer perspectives for correction.
    
    \item \textbf{Zero Fine-Tuning Approach}: Feng et al. require PEFT adaptation. CORAL operates entirely with pre-trained models, eliminating fine-tuning requirements for maximum accessibility.
    
    \item \textbf{Urdu-Specific Benchmarking}: Arif et al. establish baselines but don't explore ensemble approaches. CORAL builds on their benchmarks by implementing the multi-model LLM correction they suggest.
    
    \item \textbf{Word-Level Confidence Fusion}: Gitman et al. use utterance-level selection. CORAL extends to word-level granularity with LLM reasoning for linguistic coherence.
\end{enumerate}

\subsection{CORAL's Unique Contributions}

The CORAL framework makes several unique contributions to the field:

\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item \textbf{First Confidence-Guided LLM Fusion for Low-Resource ASR}: Combines explicit word-level confidence scores with black-box LLM reasoning for Urdu ASR correction.
    
    \item \textbf{Zero-Shot Multi-Model Correction}: Operates entirely with pre-trained models without requiring fine-tuning, calibration tuning, or supervised confidence training.
    
    \item \textbf{Practical Deployment Architecture}: Balances performance and computational efficiency through text-only LLM processing and efficient model ensemble management.
    
    \item \textbf{Comprehensive Evaluation Framework}: Establishes baseline metrics (WER, CER, confidence, ECE) for Urdu ASR ensemble systems.
    
    \item \textbf{Multi-Model Diversity}: Leverages hypotheses from architecturally diverse models rather than N-best from single model.
    
    \item \textbf{Word-Level Granularity}: Provides confidence-annotated hypotheses at word level for fine-grained correction.
\end{itemize}

\section{Theoretical Foundation}

The CORAL architecture is grounded in three key theoretical principles:

\subsection{Ensemble Learning Theory}

Ensemble methods achieve superior performance by combining predictions from multiple diverse models. The effectiveness depends on:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Model Diversity}: Different models make different errors due to architectural differences, training data, and optimization objectives.
    
    \item \textbf{Complementary Strengths}: Each model excels in different acoustic conditions, linguistic contexts, or phonetic patterns.
    
    \item \textbf{Error Reduction}: If models are uncorrelated, ensemble combination can reduce overall error rate.
\end{enumerate}

CORAL leverages diversity across:
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Architecture types (encoder-decoder vs. CTC)
    \item Model sizes (244M to 1.5B parameters)
    \item Training paradigms (supervised multilingual vs. self-supervised + fine-tuning)
\end{itemize}

\subsection{Confidence-Based Decision Making}

Uncertainty quantification is crucial for reliable ASR systems. Confidence scores provide:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Uncertainty Estimates}: Explicit indication of model confidence in predictions
    \item \textbf{Error Detection}: Low confidence correlates with higher error likelihood
    \item \textbf{Selective Correction}: High-confidence predictions can be trusted; low-confidence predictions should be scrutinized
\end{enumerate}

CORAL extracts and utilizes confidence scores to:
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Guide LLM's hypothesis selection
    \item Weight multiple predictions
    \item Identify ambiguous regions requiring careful reasoning
\end{itemize}

\subsection{Large Language Model Reasoning}

Modern instruction-tuned LLMs possess:

\begin{enumerate}[topsep=6pt,itemsep=3pt]
    \item \textbf{Linguistic Knowledge}: Deep understanding of grammar, semantics, and context
    \item \textbf{Instruction Following}: Ability to perform tasks specified in natural language prompts
    \item \textbf{Reasoning Capabilities}: Can weigh evidence, resolve ambiguities, and make coherent decisions
\end{enumerate}

CORAL leverages LLM capabilities to:
\begin{itemize}[topsep=4pt,itemsep=2pt]
    \item Synthesize multiple hypotheses into linguistically coherent output
    \item Consider confidence scores alongside linguistic plausibility
    \item Maintain contextual consistency across transcription
\end{itemize}

\section{Summary}

This literature review establishes that while significant progress has been made in ASR error correction through LLM-based post-editing, confidence estimation, and ensemble methods, critical gaps remain for low-resource languages like Urdu. Existing approaches either require expensive fine-tuning (Prakash et al.), supervised training (Nagarathna et al.), domain-specific engineering (Koilakuntla et al., Naqvi \& Tahir), lack modern LLM-based reasoning (Parikh et al., Feng et al., Gitman et al.), or focus on single-model approaches (Ma et al., Arif et al.).

CORAL uniquely addresses these gaps by combining confidence-guided multi-model fusion with black-box instruction-tuned LLMs, specifically targeting Urdu ASR without requiring fine-tuning, calibration tuning, or domain-specific data collection. The theoretical foundations in ensemble learning, confidence-based decision making, and LLM reasoning provide a solid basis for the proposed architecture.

The comprehensive benchmarking by Arif et al. provides baseline models and datasets, while the efficiency considerations from Hori et al., the confidence-based selection from Gitman et al., and the N-best correction insights from Ma et al. inform CORAL's design decisions. The parameter-efficient approaches from Feng et al. and low-resource ensemble methods from Parikh et al. validate the ensemble strategy for resource-constrained scenarios.

Iteration 1's successful implementation of the multi-model hypothesis generation pipeline with confidence extraction validates the feasibility of the first stage of this approach, setting the foundation for the instruction-guided correction mechanism to be implemented in Iteration 2.