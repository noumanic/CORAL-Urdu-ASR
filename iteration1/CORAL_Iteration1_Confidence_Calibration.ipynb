{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fa54e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'coral'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 568\u001b[39m\n\u001b[32m    565\u001b[39m file_path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:/Users/Nouman Hafeez\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDesktop/CORAL-Urdu-ASR\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdataset/cv-corpus-22.0-delta-2025-06-20/ur/clips/common_voice_ur_42810146.mp3\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# Run evaluation with 50 samples (increase for final evaluation)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m results = \u001b[43mrun_iteration1_evaluation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 507\u001b[39m, in \u001b[36mrun_iteration1_evaluation\u001b[39m\u001b[34m(dataset_path, max_samples)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_iteration1_evaluation\u001b[39m(dataset_path: \u001b[38;5;28mstr\u001b[39m, max_samples: \u001b[38;5;28mint\u001b[39m = \u001b[32m50\u001b[39m):\n\u001b[32m    500\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    501\u001b[39m \u001b[33;03m    Complete Iteration 1 evaluation pipeline\u001b[39;00m\n\u001b[32m    502\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m \u001b[33;03m        max_samples: Number of test samples (start small for testing)\u001b[39;00m\n\u001b[32m    506\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m507\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcoral\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m UrduASRWrapper  \u001b[38;5;66;03m# Import your wrapper\u001b[39;00m\n\u001b[32m    509\u001b[39m     \u001b[38;5;66;03m# Initialize\u001b[39;00m\n\u001b[32m    510\u001b[39m     asr_wrapper = UrduASRWrapper(device=\u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Use 'cuda' if available\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'coral'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Iteration: 1 - CORAL-Urdu-ASR - CORAL_Iteration1_Confidence_Caliberation.ipynb\n",
    "==================================================================\n",
    "Implements:\n",
    "1. WER/CER computation\n",
    "2. Confidence calibration metrics (ECE, MCE, Reliability diagrams)\n",
    "3. Benchmark dataset integration (Mozilla Common Voice Urdu)\n",
    "4. Baseline establishment across all models\n",
    "5. Error analysis and categorization\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import json\n",
    "from dataclasses import dataclass, asdict\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import editdistance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import your existing wrapper\n",
    "# from coral import UrduASRWrapper\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TranscriptionResult:\n",
    "    \"\"\"Store transcription results with metadata\"\"\"\n",
    "    audio_id: str\n",
    "    reference: str\n",
    "    hypothesis: str\n",
    "    word_probs: List[Tuple[str, float]]\n",
    "    model_name: str\n",
    "    wer: float\n",
    "    cer: float\n",
    "    duration: float\n",
    "    avg_confidence: float\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CalibrationMetrics:\n",
    "    \"\"\"Confidence calibration metrics\"\"\"\n",
    "    ece: float  # Expected Calibration Error\n",
    "    mce: float  # Maximum Calibration Error\n",
    "    ace: float  # Average Calibration Error\n",
    "    brier_score: float\n",
    "    confidence_accuracy_correlation: float\n",
    "    reliability_bins: Dict[str, List[float]]\n",
    "\n",
    "\n",
    "class WERCalculator:\n",
    "    \"\"\"Compute Word Error Rate and Character Error Rate\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        \"\"\"Normalize Urdu text for comparison\"\"\"\n",
    "        # Remove extra whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        # Convert to lowercase (if using Roman Urdu)\n",
    "        # text = text.lower()\n",
    "        return text.strip()\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_wer(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"\n",
    "        Compute Word Error Rate\n",
    "        WER = (S + D + I) / N\n",
    "        where S=substitutions, D=deletions, I=insertions, N=reference words\n",
    "        \"\"\"\n",
    "        ref_words = WERCalculator.normalize_text(reference).split()\n",
    "        hyp_words = WERCalculator.normalize_text(hypothesis).split()\n",
    "        \n",
    "        if len(ref_words) == 0:\n",
    "            return 0.0 if len(hyp_words) == 0 else 1.0\n",
    "        \n",
    "        distance = editdistance.eval(ref_words, hyp_words)\n",
    "        wer = distance / len(ref_words)\n",
    "        return wer\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_cer(reference: str, hypothesis: str) -> float:\n",
    "        \"\"\"Compute Character Error Rate\"\"\"\n",
    "        ref_chars = list(WERCalculator.normalize_text(reference))\n",
    "        hyp_chars = list(WERCalculator.normalize_text(hypothesis))\n",
    "        \n",
    "        if len(ref_chars) == 0:\n",
    "            return 0.0 if len(hyp_chars) == 0 else 1.0\n",
    "        \n",
    "        distance = editdistance.eval(ref_chars, hyp_chars)\n",
    "        cer = distance / len(ref_chars)\n",
    "        return cer\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_errors(reference: str, hypothesis: str) -> Dict[str, int]:\n",
    "        \"\"\"Detailed error analysis\"\"\"\n",
    "        ref_words = WERCalculator.normalize_text(reference).split()\n",
    "        hyp_words = WERCalculator.normalize_text(hypothesis).split()\n",
    "        \n",
    "        # Count error types\n",
    "        errors = {\n",
    "            'substitutions': 0,\n",
    "            'deletions': 0,\n",
    "            'insertions': 0,\n",
    "            'correct': 0\n",
    "        }\n",
    "        \n",
    "        # Simple alignment-based counting\n",
    "        distance = editdistance.eval(ref_words, hyp_words)\n",
    "        len_diff = abs(len(ref_words) - len(hyp_words))\n",
    "        \n",
    "        if len(hyp_words) > len(ref_words):\n",
    "            errors['insertions'] = len(hyp_words) - len(ref_words)\n",
    "        elif len(hyp_words) < len(ref_words):\n",
    "            errors['deletions'] = len(ref_words) - len(hyp_words)\n",
    "        \n",
    "        errors['substitutions'] = distance - len_diff\n",
    "        errors['correct'] = len(ref_words) - (errors['substitutions'] + errors['deletions'])\n",
    "        \n",
    "        return errors\n",
    "\n",
    "\n",
    "class ConfidenceCalibrator:\n",
    "    \"\"\"Analyze and calibrate confidence scores\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_ece(confidences: np.ndarray, accuracies: np.ndarray, n_bins: int = 10) -> float:\n",
    "        \"\"\"\n",
    "        Expected Calibration Error (ECE)\n",
    "        Measures average difference between confidence and accuracy across bins\n",
    "        \"\"\"\n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "        ece = 0.0\n",
    "        \n",
    "        for i in range(n_bins):\n",
    "            bin_lower = bin_boundaries[i]\n",
    "            bin_upper = bin_boundaries[i + 1]\n",
    "            \n",
    "            in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "            prop_in_bin = in_bin.mean()\n",
    "            \n",
    "            if prop_in_bin > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "        \n",
    "        return ece\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_mce(confidences: np.ndarray, accuracies: np.ndarray, n_bins: int = 10) -> float:\n",
    "        \"\"\"Maximum Calibration Error (MCE)\"\"\"\n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "        ce_bins = []\n",
    "        \n",
    "        for i in range(n_bins):\n",
    "            bin_lower = bin_boundaries[i]\n",
    "            bin_upper = bin_boundaries[i + 1]\n",
    "            \n",
    "            in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "            \n",
    "            if in_bin.sum() > 0:\n",
    "                accuracy_in_bin = accuracies[in_bin].mean()\n",
    "                avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "                ce_bins.append(np.abs(avg_confidence_in_bin - accuracy_in_bin))\n",
    "        \n",
    "        return max(ce_bins) if ce_bins else 0.0\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_brier_score(confidences: np.ndarray, accuracies: np.ndarray) -> float:\n",
    "        \"\"\"Brier Score - measure of prediction accuracy\"\"\"\n",
    "        return mean_squared_error(accuracies, confidences)\n",
    "    \n",
    "    @staticmethod\n",
    "    def reliability_diagram_data(confidences: np.ndarray, accuracies: np.ndarray, n_bins: int = 10) -> Dict:\n",
    "        \"\"\"Prepare data for reliability diagram\"\"\"\n",
    "        bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "        bin_data = {\n",
    "            'confidence_bins': [],\n",
    "            'accuracies': [],\n",
    "            'counts': []\n",
    "        }\n",
    "        \n",
    "        for i in range(n_bins):\n",
    "            bin_lower = bin_boundaries[i]\n",
    "            bin_upper = bin_boundaries[i + 1]\n",
    "            \n",
    "            in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "            \n",
    "            if in_bin.sum() > 0:\n",
    "                bin_data['confidence_bins'].append((bin_lower + bin_upper) / 2)\n",
    "                bin_data['accuracies'].append(accuracies[in_bin].mean())\n",
    "                bin_data['counts'].append(in_bin.sum())\n",
    "        \n",
    "        return bin_data\n",
    "    \n",
    "    @staticmethod\n",
    "    def analyze_calibration(word_probs: List[Tuple[str, float]], \n",
    "                          reference: str, \n",
    "                          hypothesis: str) -> CalibrationMetrics:\n",
    "        \"\"\"Complete calibration analysis\"\"\"\n",
    "        ref_words = reference.split()\n",
    "        hyp_words = [w for w, p in word_probs]\n",
    "        \n",
    "        # Align words and compute accuracies\n",
    "        confidences = []\n",
    "        accuracies = []\n",
    "        \n",
    "        for i, (word, conf) in enumerate(word_probs):\n",
    "            confidences.append(conf)\n",
    "            # Simple accuracy: 1 if word matches reference at position, 0 otherwise\n",
    "            if i < len(ref_words) and word == ref_words[i]:\n",
    "                accuracies.append(1.0)\n",
    "            else:\n",
    "                accuracies.append(0.0)\n",
    "        \n",
    "        confidences = np.array(confidences)\n",
    "        accuracies = np.array(accuracies)\n",
    "        \n",
    "        # Compute metrics\n",
    "        ece = ConfidenceCalibrator.compute_ece(confidences, accuracies)\n",
    "        mce = ConfidenceCalibrator.compute_mce(confidences, accuracies)\n",
    "        ace = mean_absolute_error(accuracies, confidences)\n",
    "        brier = ConfidenceCalibrator.compute_brier_score(confidences, accuracies)\n",
    "        correlation = np.corrcoef(confidences, accuracies)[0, 1] if len(confidences) > 1 else 0.0\n",
    "        \n",
    "        reliability_bins = ConfidenceCalibrator.reliability_diagram_data(confidences, accuracies)\n",
    "        \n",
    "        return CalibrationMetrics(\n",
    "            ece=ece,\n",
    "            mce=mce,\n",
    "            ace=ace,\n",
    "            brier_score=brier,\n",
    "            confidence_accuracy_correlation=correlation,\n",
    "            reliability_bins=reliability_bins\n",
    "        )\n",
    "\n",
    "\n",
    "class BenchmarkEvaluator:\n",
    "    \"\"\"Evaluate models on benchmark datasets\"\"\"\n",
    "    \n",
    "    def __init__(self, asr_wrapper, output_dir: str = \"./eval_results\"):\n",
    "        self.asr_wrapper = asr_wrapper\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        self.results = []\n",
    "    \n",
    "    def load_common_voice_dataset(self, dataset_path: str, split: str = \"test\", max_samples: int = 100):\n",
    "        \"\"\"\n",
    "        Load Mozilla Common Voice Urdu dataset\n",
    "        Expected structure:\n",
    "        dataset_path/\n",
    "            clips/\n",
    "                audio1.mp3\n",
    "                audio2.mp3\n",
    "            test.tsv (or validated.tsv)\n",
    "        \"\"\"\n",
    "        import csv\n",
    "        \n",
    "        dataset_path = Path(dataset_path)\n",
    "        \n",
    "        # Load metadata\n",
    "        tsv_file = dataset_path / f\"{split}.tsv\"\n",
    "        if not tsv_file.exists():\n",
    "            tsv_file = dataset_path / \"validated.tsv\"\n",
    "        \n",
    "        samples = []\n",
    "        with open(tsv_file, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f, delimiter='\\t')\n",
    "            for i, row in enumerate(reader):\n",
    "                if i >= max_samples:\n",
    "                    break\n",
    "                \n",
    "                audio_path = dataset_path / \"clips\" / row['path']\n",
    "                if audio_path.exists():\n",
    "                    samples.append({\n",
    "                        'audio_id': row['path'],\n",
    "                        'audio_path': str(audio_path),\n",
    "                        'reference': row['sentence']\n",
    "                    })\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def evaluate_model(self, model_name: str, test_samples: List[Dict]) -> List[TranscriptionResult]:\n",
    "        \"\"\"Evaluate single model on test samples\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        for sample in tqdm(test_samples, desc=f\"Processing {model_name}\"):\n",
    "            try:\n",
    "                # Transcribe\n",
    "                word_probs = self.asr_wrapper.word_probabilities(\n",
    "                    sample['audio_path'], \n",
    "                    model_name\n",
    "                )\n",
    "                \n",
    "                hypothesis = ' '.join([w for w, p in word_probs])\n",
    "                \n",
    "                # Compute metrics\n",
    "                wer = WERCalculator.compute_wer(sample['reference'], hypothesis)\n",
    "                cer = WERCalculator.compute_cer(sample['reference'], hypothesis)\n",
    "                avg_conf = np.mean([p for w, p in word_probs]) if word_probs else 0.0\n",
    "                \n",
    "                # Get audio duration\n",
    "                import librosa\n",
    "                audio, sr = librosa.load(sample['audio_path'], sr=16000)\n",
    "                duration = len(audio) / sr\n",
    "                \n",
    "                result = TranscriptionResult(\n",
    "                    audio_id=sample['audio_id'],\n",
    "                    reference=sample['reference'],\n",
    "                    hypothesis=hypothesis,\n",
    "                    word_probs=word_probs,\n",
    "                    model_name=model_name,\n",
    "                    wer=wer,\n",
    "                    cer=cer,\n",
    "                    duration=duration,\n",
    "                    avg_confidence=avg_conf\n",
    "                )\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sample['audio_id']}: {str(e)}\")\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def evaluate_all_models(self, test_samples: List[Dict], models: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"Evaluate all models and compile results\"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        for model in models:\n",
    "            model_results = self.evaluate_model(model, test_samples)\n",
    "            all_results.extend(model_results)\n",
    "            self.results.extend(model_results)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame([asdict(r) for r in all_results])\n",
    "        \n",
    "        # Save results\n",
    "        output_file = self.output_dir / \"evaluation_results.csv\"\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "        print(f\"\\nResults saved to {output_file}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def compute_aggregate_metrics(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Compute aggregate metrics per model\"\"\"\n",
    "        aggregate = df.groupby('model_name').agg({\n",
    "            'wer': ['mean', 'std', 'min', 'max'],\n",
    "            'cer': ['mean', 'std', 'min', 'max'],\n",
    "            'avg_confidence': ['mean', 'std'],\n",
    "            'duration': 'sum'\n",
    "        }).round(4)\n",
    "        \n",
    "        # Save aggregate metrics\n",
    "        output_file = self.output_dir / \"aggregate_metrics.csv\"\n",
    "        aggregate.to_csv(output_file)\n",
    "        print(f\"Aggregate metrics saved to {output_file}\")\n",
    "        \n",
    "        return aggregate\n",
    "    \n",
    "    def analyze_confidence_calibration_all(self) -> pd.DataFrame:\n",
    "        \"\"\"Analyze confidence calibration for all results\"\"\"\n",
    "        calibration_results = []\n",
    "        \n",
    "        for result in self.results:\n",
    "            cal_metrics = ConfidenceCalibrator.analyze_calibration(\n",
    "                result.word_probs,\n",
    "                result.reference,\n",
    "                result.hypothesis\n",
    "            )\n",
    "            \n",
    "            calibration_results.append({\n",
    "                'model_name': result.model_name,\n",
    "                'audio_id': result.audio_id,\n",
    "                'ece': cal_metrics.ece,\n",
    "                'mce': cal_metrics.mce,\n",
    "                'ace': cal_metrics.ace,\n",
    "                'brier_score': cal_metrics.brier_score,\n",
    "                'correlation': cal_metrics.confidence_accuracy_correlation\n",
    "            })\n",
    "        \n",
    "        df_cal = pd.DataFrame(calibration_results)\n",
    "        \n",
    "        # Aggregate by model\n",
    "        cal_aggregate = df_cal.groupby('model_name').agg({\n",
    "            'ece': ['mean', 'std'],\n",
    "            'mce': ['mean', 'std'],\n",
    "            'ace': ['mean', 'std'],\n",
    "            'brier_score': ['mean', 'std'],\n",
    "            'correlation': ['mean', 'std']\n",
    "        }).round(4)\n",
    "        \n",
    "        # Save\n",
    "        output_file = self.output_dir / \"calibration_metrics.csv\"\n",
    "        cal_aggregate.to_csv(output_file)\n",
    "        print(f\"Calibration metrics saved to {output_file}\")\n",
    "        \n",
    "        return cal_aggregate\n",
    "    \n",
    "    def generate_visualizations(self, df: pd.DataFrame):\n",
    "        \"\"\"Generate comprehensive visualizations\"\"\"\n",
    "        # 1. WER comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        model_wer = df.groupby('model_name')['wer'].mean().sort_values()\n",
    "        plt.barh(model_wer.index, model_wer.values)\n",
    "        plt.xlabel('Word Error Rate (WER)')\n",
    "        plt.title('Model Comparison: Average WER')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'wer_comparison.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 2. WER vs Confidence scatter\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for model in df['model_name'].unique():\n",
    "            model_data = df[df['model_name'] == model]\n",
    "            plt.scatter(model_data['avg_confidence'], model_data['wer'], \n",
    "                       label=model, alpha=0.6)\n",
    "        plt.xlabel('Average Confidence')\n",
    "        plt.ylabel('WER')\n",
    "        plt.title('WER vs Confidence by Model')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'wer_vs_confidence.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # 3. Error distribution\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        df.boxplot(column='wer', by='model_name', figsize=(12, 6))\n",
    "        plt.ylabel('WER')\n",
    "        plt.title('WER Distribution by Model')\n",
    "        plt.suptitle('')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(self.output_dir / 'wer_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Visualizations saved to {self.output_dir}\")\n",
    "    \n",
    "    def generate_report(self, df: pd.DataFrame):\n",
    "        \"\"\"Generate comprehensive evaluation report\"\"\"\n",
    "        report_path = self.output_dir / \"iteration1_report.txt\"\n",
    "        \n",
    "        with open(report_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"CORAL ITERATION 1: BASELINE EVALUATION REPORT\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Total samples evaluated: {len(df)}\\n\")\n",
    "            f.write(f\"Models evaluated: {df['model_name'].nunique()}\\n\")\n",
    "            f.write(f\"Total audio duration: {df['duration'].sum():.2f} seconds\\n\\n\")\n",
    "            \n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(\"AGGREGATE METRICS BY MODEL\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            aggregate = self.compute_aggregate_metrics(df)\n",
    "            f.write(aggregate.to_string())\n",
    "            f.write(\"\\n\\n\")\n",
    "            \n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(\"BEST PERFORMING MODELS\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            best_wer = df.groupby('model_name')['wer'].mean().idxmin()\n",
    "            best_wer_value = df.groupby('model_name')['wer'].mean().min()\n",
    "            f.write(f\"Best WER: {best_wer} ({best_wer_value:.4f})\\n\\n\")\n",
    "            \n",
    "            best_cer = df.groupby('model_name')['cer'].mean().idxmin()\n",
    "            best_cer_value = df.groupby('model_name')['cer'].mean().min()\n",
    "            f.write(f\"Best CER: {best_cer} ({best_cer_value:.4f})\\n\\n\")\n",
    "            \n",
    "            f.write(\"-\"*80 + \"\\n\")\n",
    "            f.write(\"CALIBRATION ANALYSIS\\n\")\n",
    "            f.write(\"-\"*80 + \"\\n\\n\")\n",
    "            \n",
    "            cal_metrics = self.analyze_confidence_calibration_all()\n",
    "            f.write(cal_metrics.to_string())\n",
    "            f.write(\"\\n\\n\")\n",
    "            \n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "            f.write(\"Report generation complete\\n\")\n",
    "            f.write(\"=\"*80 + \"\\n\")\n",
    "        \n",
    "        print(f\"\\nComprehensive report saved to {report_path}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "def run_iteration1_evaluation(dataset_path: str, max_samples: int = 50):\n",
    "    \"\"\"\n",
    "    Complete Iteration 1 evaluation pipeline\n",
    "    \n",
    "    Args:\n",
    "        dataset_path: Path to Common Voice Urdu dataset\n",
    "        max_samples: Number of test samples (start small for testing)\n",
    "    \"\"\"\n",
    "# ============================================================================\n",
    "# UrduASRWrapper Class Definition (from CORAL_Iteration1_ASR_Ensemble.ipynb)\n",
    "# ============================================================================\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    Wav2Vec2Processor, Wav2Vec2ForCTC,\n",
    "    SeamlessM4TForSpeechToText, SeamlessM4TProcessor,\n",
    "    AutoProcessor, AutoModelForCTC\n",
    ")\n",
    "\n",
    "class UrduASRWrapper:\n",
    "    \"\"\"Unified wrapper for multiple Urdu ASR models.\"\"\"\n",
    "    \n",
    "    SUPPORTED_MODELS = {\n",
    "        \"whisper-large\": \"openai/whisper-large-v3\",\n",
    "        \"whisper-medium\": \"openai/whisper-medium\",\n",
    "        \"whisper-small\": \"openai/whisper-small\",\n",
    "        \"seamless-large\": \"facebook/seamless-m4t-v2-large\",\n",
    "        \"seamless-medium\": \"facebook/seamless-m4t-medium\",\n",
    "        \"mms-1b\": \"facebook/mms-1b-all\",\n",
    "        \"mms-300m\": \"facebook/mms-300m\",\n",
    "        \"wav2vec2-urdu\": \"kingabzpro/wav2vec2-large-xls-r-300m-Urdu\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, device: str = None):\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        print(f\"ASR Wrapper initialized on: {self.device}\")\n",
    "        \n",
    "        self.current_model = None\n",
    "        self.processor = None\n",
    "        self.current_model_name = None\n",
    "    \n",
    "    def _preprocess_audio(self, file_path: str, target_sr: int = 16000) -> np.ndarray:\n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "            \n",
    "            if audio.dtype != np.float32:\n",
    "                audio = audio.astype(np.float32)\n",
    "            \n",
    "            max_val = np.abs(audio).max()\n",
    "            if max_val > 0:\n",
    "                audio = audio / max_val\n",
    "            \n",
    "            return audio\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading audio file {file_path}: {str(e)}\")\n",
    "    \n",
    "    def _load_model(self, model_name: str):\n",
    "        if model_name not in self.SUPPORTED_MODELS:\n",
    "            raise ValueError(f\"Model {model_name} not supported. Choose from: {list(self.SUPPORTED_MODELS.keys())}\")\n",
    "        \n",
    "        model_id = self.SUPPORTED_MODELS[model_name]\n",
    "        print(f\"Loading {model_name} ({model_id})...\")\n",
    "        \n",
    "        try:\n",
    "            if \"whisper\" in model_name:\n",
    "                self.processor = WhisperProcessor.from_pretrained(model_id)\n",
    "                self.current_model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "                \n",
    "            elif \"seamless\" in model_name:\n",
    "                self.processor = SeamlessM4TProcessor.from_pretrained(model_id)\n",
    "                self.current_model = SeamlessM4TForSpeechToText.from_pretrained(model_id)\n",
    "                \n",
    "            elif \"mms\" in model_name:\n",
    "                self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "                self.current_model = AutoModelForCTC.from_pretrained(model_id)\n",
    "                \n",
    "            elif \"wav2vec2\" in model_name:\n",
    "                self.processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "                self.current_model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "            \n",
    "            self.current_model = self.current_model.to(self.device)\n",
    "            self.current_model.eval()\n",
    "            self.current_model_name = model_name\n",
    "            \n",
    "            print(f\"{model_name} loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model {model_name}: {str(e)}\")\n",
    "    \n",
    "    def _extract_whisper_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n",
    "        input_features = self.processor(\n",
    "            audio_array, \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted_ids = self.current_model.generate(\n",
    "                input_features,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        transcription = self.processor.batch_decode(\n",
    "            predicted_ids.sequences, \n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        word_probs = []\n",
    "        if hasattr(predicted_ids, 'scores') and predicted_ids.scores:\n",
    "            all_probs = []\n",
    "            for score in predicted_ids.scores:\n",
    "                probs = torch.softmax(score, dim=-1)\n",
    "                max_prob = probs.max().item()\n",
    "                all_probs.append(max_prob)\n",
    "            \n",
    "            words = transcription.strip().split()\n",
    "            \n",
    "            if len(words) > 0 and len(all_probs) > 0:\n",
    "                avg_prob = np.mean(all_probs)\n",
    "                word_probs = [(word, avg_prob) for word in words]\n",
    "            else:\n",
    "                word_probs = [(word, 0.5) for word in words]\n",
    "        else:\n",
    "            words = transcription.strip().split()\n",
    "            word_probs = [(word, 0.8) for word in words]\n",
    "        \n",
    "        return word_probs\n",
    "    \n",
    "    def _extract_ctc_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n",
    "        inputs = self.processor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        input_values = inputs.input_values.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.current_model(input_values).logits\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        words = transcription.strip().split()\n",
    "        word_probs = []\n",
    "        \n",
    "        if len(words) > 0:\n",
    "            max_probs = probs.max(dim=-1).values.squeeze()\n",
    "            avg_confidence = max_probs.mean().item()\n",
    "            word_probs = [(word, avg_confidence) for word in words]\n",
    "        \n",
    "        return word_probs\n",
    "    \n",
    "    def _extract_seamless_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n",
    "        audio_inputs = self.processor(\n",
    "            audios=audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.current_model.generate(\n",
    "                **audio_inputs,\n",
    "                tgt_lang=\"urd\",\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        transcription = self.processor.decode(\n",
    "            output.sequences[0].tolist(),\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        word_probs = []\n",
    "        if hasattr(output, 'scores') and output.scores:\n",
    "            all_probs = []\n",
    "            for score in output.scores:\n",
    "                probs = torch.softmax(score, dim=-1)\n",
    "                max_prob = probs.max().item()\n",
    "                all_probs.append(max_prob)\n",
    "            \n",
    "            words = transcription.strip().split()\n",
    "            if len(words) > 0 and len(all_probs) > 0:\n",
    "                avg_prob = np.mean(all_probs)\n",
    "                word_probs = [(word, avg_prob) for word in words]\n",
    "            else:\n",
    "                word_probs = [(word, 0.7) for word in words]\n",
    "        else:\n",
    "            words = transcription.strip().split()\n",
    "            word_probs = [(word, 0.7) for word in words]\n",
    "        \n",
    "        return word_probs\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        if self.current_model is not None:\n",
    "            del self.current_model\n",
    "            self.current_model = None\n",
    "        \n",
    "        if self.processor is not None:\n",
    "            del self.processor\n",
    "            self.processor = None\n",
    "        \n",
    "        self.current_model_name = None\n",
    "        \n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    def word_probabilities(self, audio_file_path: str, model_name: str) -> List[Tuple[str, float]]:\n",
    "        try:\n",
    "            audio_array = self._preprocess_audio(audio_file_path)\n",
    "            self._load_model(model_name)\n",
    "            \n",
    "            if \"whisper\" in model_name:\n",
    "                results = self._extract_whisper_probabilities(audio_array)\n",
    "            elif \"mms\" in model_name or \"wav2vec2\" in model_name:\n",
    "                results = self._extract_ctc_probabilities(audio_array)\n",
    "            elif \"seamless\" in model_name:\n",
    "                results = self._extract_seamless_probabilities(audio_array)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "            \n",
    "            self._cleanup()\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._cleanup()\n",
    "            raise RuntimeError(f\"Error processing audio with {model_name}: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# End of UrduASRWrapper Class\n",
    "# ============================================================================\n",
    "    \n",
    "    # Initialize\n",
    "    asr_wrapper = UrduASRWrapper(device='cpu')  # Use 'cuda' if available\n",
    "    evaluator = BenchmarkEvaluator(asr_wrapper, output_dir=\"./iteration1_results\")\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"Loading test dataset...\")\n",
    "    test_samples = evaluator.load_common_voice_dataset(dataset_path, max_samples=max_samples)\n",
    "    print(f\"Loaded {len(test_samples)} test samples\")\n",
    "    \n",
    "    # Models to evaluate (start with smaller models for speed)\n",
    "    models_to_test = [\n",
    "        \"whisper-small\",\n",
    "        \"whisper-medium\",\n",
    "        \"wav2vec2-urdu\",\n",
    "        \"mms-300m\"\n",
    "    ]\n",
    "    \n",
    "    # Run evaluation\n",
    "    print(\"\\nStarting evaluation...\")\n",
    "    results_df = evaluator.evaluate_all_models(test_samples, models_to_test)\n",
    "    \n",
    "    # Compute metrics\n",
    "    print(\"\\nComputing aggregate metrics...\")\n",
    "    evaluator.compute_aggregate_metrics(results_df)\n",
    "    \n",
    "    # Analyze calibration\n",
    "    print(\"\\nAnalyzing confidence calibration...\")\n",
    "    evaluator.analyze_confidence_calibration_all()\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(\"\\nGenerating visualizations...\")\n",
    "    evaluator.generate_visualizations(results_df)\n",
    "    \n",
    "    # Generate report\n",
    "    print(\"\\nGenerating final report...\")\n",
    "    evaluator.generate_report(results_df)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ITERATION 1 EVALUATION COMPLETE!\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Results directory: {evaluator.output_dir}\")\n",
    "    print(\"\\nDeliverables:\")\n",
    "    print(\"  - evaluation_results.csv: Detailed per-sample results\")\n",
    "    print(\"  - aggregate_metrics.csv: Model performance summary\")\n",
    "    print(\"  - calibration_metrics.csv: Confidence calibration analysis\")\n",
    "    print(\"  - wer_comparison.png: Model WER comparison\")\n",
    "    print(\"  - wer_vs_confidence.png: Confidence vs accuracy analysis\")\n",
    "    print(\"  - wer_distribution.png: Error distribution visualization\")\n",
    "    print(\"  - iteration1_report.txt: Comprehensive evaluation report\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    file_path = r\"C:/Users/Nouman Hafeez\\Desktop/CORAL-Urdu-ASR\\dataset/cv-corpus-22.0-delta-2025-06-20/ur/clips/common_voice_ur_42810146.mp3\"\n",
    "    \n",
    "    # Run evaluation with 50 samples (increase for final evaluation)\n",
    "    results = run_iteration1_evaluation(file_path, max_samples=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coral-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
