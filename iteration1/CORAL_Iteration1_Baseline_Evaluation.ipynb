{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef397f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing dependencies...\n",
      "Loading libraries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'c:\\Users\\Nouman' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "c:\\Users\\Nouman Hafeez\\Desktop\\CORAL-Urdu-ASR\\coral-venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: CPU\n",
      "\n",
      "Configuration:\n",
      "  DATASET_PATH: C:\\Users\\Nouman Hafeez\\Desktop\\FYP\\dataset\\cv-corpus-22.0-delta-2025-06-20\\ur\n",
      "  MAX_SAMPLES: 10\n",
      "  OUTPUT_DIR: ./iteration1_results\n",
      "  MODELS: ['whisper-small', 'whisper-medium', 'whisper-large', 'wav2vec2-urdu', 'mms-300m']\n",
      "  DEVICE: cpu\n",
      "  BATCH_CLEANUP: True\n",
      "\n",
      "================================================================================\n",
      "CORAL ITERATION 1: BASELINE EVALUATION\n",
      "================================================================================\n",
      "\n",
      "Timestamp: 2025-10-05 11:34:18\n",
      "\n",
      "[1/5] Loading test dataset from C:\\Users\\Nouman Hafeez\\Desktop\\FYP\\dataset\\cv-corpus-22.0-delta-2025-06-20\\ur...\n",
      "\n",
      "\n",
      "ERROR: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Nouman Hafeez\\\\Desktop\\\\FYP\\\\dataset\\\\cv-corpus-22.0-delta-2025-06-20\\\\ur\\\\validated.tsv'\n",
      "\n",
      "Troubleshooting steps:\n",
      "1. Verify DATASET_PATH in CONFIG\n",
      "2. Check dataset structure (clips/ folder and .tsv files)\n",
      "3. Reduce MAX_SAMPLES if running out of memory\n",
      "4. Comment out large models in CONFIG['MODELS']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Nouman Hafeez\\\\Desktop\\\\FYP\\\\dataset\\\\cv-corpus-22.0-delta-2025-06-20\\\\ur\\\\validated.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 472\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m         results_df, aggregate_metrics = \u001b[43mrun_iteration1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEvaluation successful!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    475\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mReview the results in ./iteration1_results/\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 338\u001b[39m, in \u001b[36mrun_iteration1\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[32m    337\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[1/5] Loading test dataset from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mDATASET_PATH\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m test_samples = \u001b[43mload_test_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDATASET_PATH\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mMAX_SAMPLES\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m test samples\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Initialize\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 228\u001b[39m, in \u001b[36mload_test_samples\u001b[39m\u001b[34m(dataset_path, max_samples)\u001b[39m\n\u001b[32m    225\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    227\u001b[39m samples = []\n\u001b[32m--> \u001b[39m\u001b[32m228\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    229\u001b[39m     reader = csv.DictReader(f, delimiter=\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m    230\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(reader):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nouman Hafeez\\Desktop\\CORAL-Urdu-ASR\\coral-venv\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Nouman Hafeez\\\\Desktop\\\\FYP\\\\dataset\\\\cv-corpus-22.0-delta-2025-06-20\\\\ur\\\\validated.tsv'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Iteration: 1 - CORAL-Urdu-ASR - CORAL_Iteration1_Baseline_Evaluation.ipynb\n",
    "===============================================\n",
    "Paste this entire script into a Kaggle notebook cell and run.\n",
    "Requires: Mozilla Common Voice Urdu dataset as input\n",
    "\"\"\"\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: INSTALL DEPENDENCIES\n",
    "# ============================================================================\n",
    "print(\"Installing dependencies...\")\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q editdistance\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: IMPORTS\n",
    "# ============================================================================\n",
    "print(\"Loading libraries...\")\n",
    "import torch\n",
    "import gc\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "import json\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import editdistance\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    Wav2Vec2Processor, Wav2Vec2ForCTC,\n",
    "    AutoProcessor, AutoModelForCTC\n",
    ")\n",
    "\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: CONFIGURATION\n",
    "# ============================================================================\n",
    "CONFIG = {\n",
    "DATASET_PATH = \"/kaggle/input/common-voice-ur/cv-corpus-22.0-delta-2025-06-20/ur\"\n",
    "'MAX_SAMPLES': 10,  # Start small\n",
    "'OUTPUT_DIR': './iteration1_results',\n",
    "# Models to evaluate (comment out slow models for quick testing)\n",
    "'MODELS': [\n",
    "        \"whisper-small\",      # Fast\n",
    "        \"whisper-medium\",     # Balanced\n",
    "        \"whisper-large\",    # Accurate but slow\n",
    "        \"wav2vec2-urdu\",      # Urdu-specific\n",
    "        \"mms-300m\",           # Multilingual\n",
    "    ],\n",
    "    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'BATCH_CLEANUP': True\n",
    "}\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 4: ASR WRAPPER (Streamlined)\n",
    "# ============================================================================\n",
    "\n",
    "class UrduASRWrapper:\n",
    "    SUPPORTED_MODELS = {\n",
    "        \"whisper-large\": \"openai/whisper-large-v3\",\n",
    "        \"whisper-medium\": \"openai/whisper-medium\",\n",
    "        \"whisper-small\": \"openai/whisper-small\",\n",
    "        \"mms-1b\": \"facebook/mms-1b-all\",\n",
    "        \"mms-300m\": \"facebook/mms-300m\",\n",
    "        \"wav2vec2-urdu\": \"kingabzpro/wav2vec2-large-xls-r-300m-Urdu\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, device='cpu'):\n",
    "        self.device = device\n",
    "        self.current_model = None\n",
    "        self.processor = None\n",
    "        self.current_model_name = None\n",
    "    \n",
    "    def _preprocess_audio(self, file_path: str, target_sr: int = 16000):\n",
    "        audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "        if audio.dtype != np.float32:\n",
    "            audio = audio.astype(np.float32)\n",
    "        max_val = np.abs(audio).max()\n",
    "        if max_val > 0:\n",
    "            audio = audio / max_val\n",
    "        return audio\n",
    "    \n",
    "    def _load_model(self, model_name: str):\n",
    "        if model_name not in self.SUPPORTED_MODELS:\n",
    "            raise ValueError(f\"Model {model_name} not supported\")\n",
    "        \n",
    "        model_id = self.SUPPORTED_MODELS[model_name]\n",
    "        \n",
    "        if \"whisper\" in model_name:\n",
    "            self.processor = WhisperProcessor.from_pretrained(model_id)\n",
    "            self.current_model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "        elif \"mms\" in model_name:\n",
    "            self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "            self.current_model = AutoModelForCTC.from_pretrained(model_id)\n",
    "        elif \"wav2vec2\" in model_name:\n",
    "            self.processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "            self.current_model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "        \n",
    "        self.current_model = self.current_model.to(self.device)\n",
    "        self.current_model.eval()\n",
    "        self.current_model_name = model_name\n",
    "    \n",
    "    def _extract_whisper_probs(self, audio_array):\n",
    "        input_features = self.processor(\n",
    "            audio_array, sampling_rate=16000, return_tensors=\"pt\"\n",
    "        ).input_features.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = self.current_model.generate(\n",
    "                input_features, return_dict_in_generate=True, output_scores=True\n",
    "            )\n",
    "        \n",
    "        transcription = self.processor.batch_decode(\n",
    "            output.sequences, skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        if hasattr(output, 'scores') and output.scores:\n",
    "            probs = [torch.softmax(score, dim=-1).max().item() for score in output.scores]\n",
    "            avg_prob = np.mean(probs) if probs else 0.8\n",
    "        else:\n",
    "            avg_prob = 0.8\n",
    "        \n",
    "        words = transcription.strip().split()\n",
    "        return [(word, avg_prob) for word in words]\n",
    "    \n",
    "    def _extract_ctc_probs(self, audio_array):\n",
    "        inputs = self.processor(\n",
    "            audio_array, sampling_rate=16000, return_tensors=\"pt\", padding=True\n",
    "        )\n",
    "        input_values = inputs.input_values.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.current_model(input_values).logits\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        words = transcription.strip().split()\n",
    "        if len(words) > 0:\n",
    "            avg_conf = probs.max(dim=-1).values.squeeze().mean().item()\n",
    "            return [(word, avg_conf) for word in words]\n",
    "        return []\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        if self.current_model is not None:\n",
    "            del self.current_model\n",
    "            del self.processor\n",
    "            self.current_model = None\n",
    "            self.processor = None\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    def word_probabilities(self, audio_file_path: str, model_name: str):\n",
    "        try:\n",
    "            audio_array = self._preprocess_audio(audio_file_path)\n",
    "            self._load_model(model_name)\n",
    "            \n",
    "            if \"whisper\" in model_name:\n",
    "                results = self._extract_whisper_probs(audio_array)\n",
    "            elif \"mms\" in model_name or \"wav2vec2\" in model_name:\n",
    "                results = self._extract_ctc_probs(audio_array)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "            \n",
    "            self._cleanup()\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            self._cleanup()\n",
    "            raise RuntimeError(f\"Error: {str(e)}\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 5: EVALUATION FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def compute_wer(reference: str, hypothesis: str):\n",
    "    ref_words = reference.strip().split()\n",
    "    hyp_words = hypothesis.strip().split()\n",
    "    if len(ref_words) == 0:\n",
    "        return 0.0 if len(hyp_words) == 0 else 1.0\n",
    "    return editdistance.eval(ref_words, hyp_words) / len(ref_words)\n",
    "\n",
    "def compute_cer(reference: str, hypothesis: str):\n",
    "    ref_chars = list(reference.strip())\n",
    "    hyp_chars = list(hypothesis.strip())\n",
    "    if len(ref_chars) == 0:\n",
    "        return 0.0 if len(hyp_chars) == 0 else 1.0\n",
    "    return editdistance.eval(ref_chars, hyp_chars) / len(ref_chars)\n",
    "\n",
    "def compute_ece(confidences, accuracies, n_bins=10):\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        in_bin = (confidences > bin_boundaries[i]) & (confidences <= bin_boundaries[i + 1])\n",
    "        if in_bin.sum() > 0:\n",
    "            acc = accuracies[in_bin].mean()\n",
    "            conf = confidences[in_bin].mean()\n",
    "            ece += np.abs(conf - acc) * in_bin.mean()\n",
    "    return ece\n",
    "\n",
    "def load_test_samples(dataset_path, max_samples):\n",
    "    dataset_path = Path(dataset_path)\n",
    "    \n",
    "    # Find metadata file\n",
    "    for fname in [\"test.tsv\", \"dev.tsv\", \"validated.tsv\"]:\n",
    "        tsv_file = dataset_path / fname\n",
    "        if tsv_file.exists():\n",
    "            break\n",
    "    \n",
    "    samples = []\n",
    "    with open(tsv_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= max_samples:\n",
    "                break\n",
    "            audio_path = dataset_path / \"clips\" / row['path']\n",
    "            if audio_path.exists():\n",
    "                samples.append({\n",
    "                    'audio_id': row['path'],\n",
    "                    'audio_path': str(audio_path),\n",
    "                    'reference': row['sentence'],\n",
    "                    'duration': float(row.get('duration', 0))\n",
    "                })\n",
    "    \n",
    "    return samples\n",
    "\n",
    "def evaluate_model(asr_wrapper, model_name, test_samples):\n",
    "    results = []\n",
    "    \n",
    "    for sample in tqdm(test_samples, desc=model_name):\n",
    "        try:\n",
    "            word_probs = asr_wrapper.word_probabilities(sample['audio_path'], model_name)\n",
    "            hypothesis = ' '.join([w for w, p in word_probs])\n",
    "            reference = sample['reference']\n",
    "            \n",
    "            wer = compute_wer(reference, hypothesis)\n",
    "            cer = compute_cer(reference, hypothesis)\n",
    "            avg_conf = np.mean([p for w, p in word_probs]) if word_probs else 0.0\n",
    "            \n",
    "            # Calibration\n",
    "            ref_words = reference.split()\n",
    "            confidences = [p for w, p in word_probs]\n",
    "            accuracies = [1.0 if i < len(ref_words) and w == ref_words[i] else 0.0 \n",
    "                         for i, (w, p) in enumerate(word_probs)]\n",
    "            \n",
    "            ece = compute_ece(np.array(confidences), np.array(accuracies)) if confidences else 0.0\n",
    "            \n",
    "            results.append({\n",
    "                'audio_id': sample['audio_id'],\n",
    "                'model_name': model_name,\n",
    "                'reference': reference,\n",
    "                'hypothesis': hypothesis,\n",
    "                'wer': wer,\n",
    "                'cer': cer,\n",
    "                'avg_confidence': avg_conf,\n",
    "                'ece': ece,\n",
    "                'duration': sample['duration']\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError on {sample['audio_id']}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# ============================================================================\n",
    "# PART 6: VISUALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "def generate_plots(df, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # WER comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    model_wer = df.groupby('model_name')['wer'].mean().sort_values()\n",
    "    plt.barh(model_wer.index, model_wer.values, color='steelblue')\n",
    "    plt.xlabel('Word Error Rate (WER)')\n",
    "    plt.title('Model Comparison: Average WER', fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'wer_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # WER distribution\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df.boxplot(column='wer', by='model_name')\n",
    "    plt.ylabel('WER')\n",
    "    plt.title('WER Distribution by Model', fontweight='bold')\n",
    "    plt.suptitle('')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'wer_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Calibration\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    model_ece = df.groupby('model_name')['ece'].mean().sort_values()\n",
    "    plt.barh(model_ece.index, model_ece.values, color='coral')\n",
    "    plt.xlabel('Expected Calibration Error (ECE)')\n",
    "    plt.title('Confidence Calibration by Model', fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'calibration.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================================\n",
    "# PART 7: MAIN EXECUTION\n",
    "# ============================================================================\n",
    "\n",
    "def run_iteration1():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORAL ITERATION 1: BASELINE EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # Setup\n",
    "    output_dir = Path(CONFIG['OUTPUT_DIR'])\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    # Load data\n",
    "    print(f\"\\n[1/5] Loading test dataset from {CONFIG['DATASET_PATH']}...\")\n",
    "    test_samples = load_test_samples(CONFIG['DATASET_PATH'], CONFIG['MAX_SAMPLES'])\n",
    "    print(f\"Loaded {len(test_samples)} test samples\")\n",
    "    \n",
    "    # Initialize\n",
    "    print(f\"\\n[2/5] Initializing ASR wrapper on {CONFIG['DEVICE']}...\")\n",
    "    asr_wrapper = UrduASRWrapper(device=CONFIG['DEVICE'])\n",
    "    \n",
    "    # Evaluate\n",
    "    print(f\"\\n[3/5] Evaluating {len(CONFIG['MODELS'])} models...\")\n",
    "    all_results = []\n",
    "    \n",
    "    for model in CONFIG['MODELS']:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        model_results = evaluate_model(asr_wrapper, model, test_samples)\n",
    "        all_results.extend(model_results)\n",
    "        \n",
    "        if CONFIG['BATCH_CLEANUP']:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Save detailed results\n",
    "    print(f\"\\n[4/5] Saving results...\")\n",
    "    df.to_csv(output_dir / 'detailed_results.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    # Compute aggregates\n",
    "    aggregate = df.groupby('model_name').agg({\n",
    "        'wer': ['mean', 'std', 'min', 'max'],\n",
    "        'cer': ['mean', 'std'],\n",
    "        'avg_confidence': ['mean', 'std'],\n",
    "        'ece': ['mean', 'std'],\n",
    "        'duration': 'sum'\n",
    "    }).round(4)\n",
    "    \n",
    "    aggregate.to_csv(output_dir / 'aggregate_metrics.csv')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGGREGATE METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(aggregate)\n",
    "    \n",
    "    # Generate visualizations\n",
    "    print(f\"\\n[5/5] Generating visualizations...\")\n",
    "    generate_plots(df, output_dir)\n",
    "    \n",
    "    # Generate report\n",
    "    report_file = output_dir / 'ITERATION1_REPORT.txt'\n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"CORAL PROJECT - ITERATION 1 EVALUATION REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Samples: {len(test_samples)}\\n\")\n",
    "        f.write(f\"Models: {len(CONFIG['MODELS'])}\\n\")\n",
    "        f.write(f\"Total Duration: {df['duration'].sum():.2f}s\\n\\n\")\n",
    "        \n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(\"BASELINE WER BY MODEL\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\\n\")\n",
    "        f.write(df.groupby('model_name')['wer'].describe().to_string())\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        best_model = df.groupby('model_name')['wer'].mean().idxmin()\n",
    "        best_wer = df.groupby('model_name')['wer'].mean().min()\n",
    "        f.write(f\"BEST MODEL: {best_model}\\n\")\n",
    "        f.write(f\"BASELINE WER: {best_wer:.4f} ({best_wer*100:.2f}%)\\n\\n\")\n",
    "        \n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(\"CALIBRATION ANALYSIS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\\n\")\n",
    "        f.write(df.groupby('model_name')['ece'].describe().to_string())\n",
    "        f.write(\"\\n\\n\")\n",
    "        \n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(\"ITERATION 1 DELIVERABLES - COMPLETE\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\\n\")\n",
    "        f.write(\"✓ ASR ensemble integrated\\n\")\n",
    "        f.write(\"✓ Confidence extraction implemented\\n\")\n",
    "        f.write(\"✓ Baseline WER established\\n\")\n",
    "        f.write(\"✓ Calibration metrics computed\\n\")\n",
    "        f.write(\"✓ Comparative analysis complete\\n\\n\")\n",
    "        \n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(\"NEXT STEPS (ITERATION 2)\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\\n\")\n",
    "        f.write(\"• Develop LLM instruction prompts\\n\")\n",
    "        f.write(\"• Test hypothesis fusion strategies\\n\")\n",
    "        f.write(f\"• Target WER: < {(best_wer*0.85)*100:.2f}%\\n\")\n",
    "        f.write(\"• Begin prompt engineering experiments\\n\\n\")\n",
    "        \n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ITERATION 1 COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nResults saved to: {output_dir.absolute()}\")\n",
    "    print(\"\\nGenerated files:\")\n",
    "    print(\"  • detailed_results.csv\")\n",
    "    print(\"  • aggregate_metrics.csv\")\n",
    "    print(\"  • wer_comparison.png\")\n",
    "    print(\"  • wer_distribution.png\")\n",
    "    print(\"  • calibration.png\")\n",
    "    print(\"  • ITERATION1_REPORT.txt\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"KEY FINDINGS\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Best Model: {best_model}\")\n",
    "    print(f\"Baseline WER: {best_wer*100:.2f}%\")\n",
    "    print(f\"Samples Evaluated: {len(df)}\")\n",
    "    print(f\"Average ECE: {df['ece'].mean():.4f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"READY FOR ITERATION 2\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Target WER: < {(best_wer*0.85)*100:.2f}%\")\n",
    "    print(\"Next: Develop LLM-based hypothesis correction\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return df, aggregate\n",
    "\n",
    "# ============================================================================\n",
    "# RUN THE EVALUATION\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results_df, aggregate_metrics = run_iteration1()\n",
    "        \n",
    "        print(\"\\nEvaluation successful!\")\n",
    "        print(\"Review the results in ./iteration1_results/\")\n",
    "        print(\"\\nTo analyze further:\")\n",
    "        print(\"  results_df.head()  # View sample results\")\n",
    "        print(\"  results_df.describe()  # Statistical summary\")\n",
    "        print(\"  results_df.groupby('model_name')['wer'].mean()  # WER by model\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nERROR: {str(e)}\")\n",
    "        print(\"\\nTroubleshooting steps:\")\n",
    "        print(\"1. Verify DATASET_PATH in CONFIG\")\n",
    "        print(\"2. Check dataset structure (clips/ folder and .tsv files)\")\n",
    "        print(\"3. Reduce MAX_SAMPLES if running out of memory\")\n",
    "        print(\"4. Comment out large models in CONFIG['MODELS']\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coral-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
