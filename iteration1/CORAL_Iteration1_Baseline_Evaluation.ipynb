{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2689f5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(\"Installing dependencies...\")\n",
    "!{sys.executable} -m pip install -q editdistance\n",
    "\n",
    "print(\"Loading libraries...\")\n",
    "import torch\n",
    "import gc\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "import json\n",
    "import csv\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import editdistance\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "from transformers import (\n",
    "    WhisperProcessor, WhisperForConditionalGeneration,\n",
    "    Wav2Vec2Processor, Wav2Vec2ForCTC,\n",
    "    SeamlessM4TForSpeechToText, SeamlessM4TProcessor,\n",
    "    AutoProcessor, AutoModelForCTC\n",
    ")\n",
    "\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "\n",
    "class UrduASRWrapper:\n",
    "    SUPPORTED_MODELS = {\n",
    "        \"whisper-large\": \"openai/whisper-large-v3\",\n",
    "        \"whisper-medium\": \"openai/whisper-medium\",\n",
    "        \"whisper-small\": \"openai/whisper-small\",\n",
    "        \"seamless-large\": \"facebook/seamless-m4t-v2-large\",\n",
    "        \"seamless-medium\": \"facebook/seamless-m4t-medium\",\n",
    "        \"mms-1b\": \"facebook/mms-1b-all\",\n",
    "        \"mms-300m\": \"facebook/mms-300m\",\n",
    "        \"wav2vec2-urdu\": \"kingabzpro/wav2vec2-large-xls-r-300m-Urdu\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, device: str = None, use_fp16: bool = True):\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.use_fp16 = use_fp16 and self.device == \"cuda\"\n",
    "        print(f\"ASR Wrapper initialized on: {self.device} (FP16: {self.use_fp16})\")\n",
    "        self.current_model = None\n",
    "        self.processor = None\n",
    "        self.current_model_name = None\n",
    "        self.audio_cache = {}\n",
    "    \n",
    "    def _preprocess_audio(self, file_path: str, target_sr: int = 16000) -> np.ndarray:\n",
    "        if file_path in self.audio_cache:\n",
    "            return self.audio_cache[file_path]\n",
    "        \n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "            if audio.dtype != np.float32:\n",
    "                audio = audio.astype(np.float32)\n",
    "            max_val = np.abs(audio).max()\n",
    "            if max_val > 0:\n",
    "                audio = audio / max_val\n",
    "            self.audio_cache[file_path] = audio\n",
    "            return audio\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading audio file {file_path}: {str(e)}\")\n",
    "    \n",
    "    def _load_model(self, model_name: str):\n",
    "        if self.current_model_name == model_name:\n",
    "            return\n",
    "        \n",
    "        self._cleanup()\n",
    "        \n",
    "        if model_name not in self.SUPPORTED_MODELS:\n",
    "            raise ValueError(f\"Model {model_name} not supported. Choose from: {list(self.SUPPORTED_MODELS.keys())}\")\n",
    "        \n",
    "        model_id = self.SUPPORTED_MODELS[model_name]\n",
    "        print(f\"Loading {model_name} ({model_id})...\")\n",
    "        \n",
    "        try:\n",
    "            if \"whisper\" in model_name:\n",
    "                self.processor = WhisperProcessor.from_pretrained(model_id)\n",
    "                self.current_model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "            elif \"seamless\" in model_name:\n",
    "                self.processor = SeamlessM4TProcessor.from_pretrained(model_id)\n",
    "                self.current_model = SeamlessM4TForSpeechToText.from_pretrained(model_id)\n",
    "            elif \"mms\" in model_name:\n",
    "                self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "                self.current_model = AutoModelForCTC.from_pretrained(model_id)\n",
    "            elif \"wav2vec2\" in model_name:\n",
    "                self.processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "                self.current_model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "            \n",
    "            self.current_model = self.current_model.to(self.device)\n",
    "            if self.use_fp16:\n",
    "                self.current_model = self.current_model.half()\n",
    "            self.current_model.eval()\n",
    "            self.current_model_name = model_name\n",
    "            print(f\"{model_name} loaded successfully\")\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model {model_name}: {str(e)}\")\n",
    "    \n",
    "    def _extract_whisper_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n",
    "        input_features = self.processor(audio_array, sampling_rate=16000, return_tensors=\"pt\").input_features.to(self.device)\n",
    "        if self.use_fp16:\n",
    "            input_features = input_features.half()\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            predicted_ids = self.current_model.generate(input_features, return_dict_in_generate=True, output_scores=True)\n",
    "        \n",
    "        transcription = self.processor.batch_decode(predicted_ids.sequences, skip_special_tokens=True)[0]\n",
    "        word_probs = []\n",
    "        \n",
    "        if hasattr(predicted_ids, 'scores') and predicted_ids.scores:\n",
    "            all_probs = []\n",
    "            for score in predicted_ids.scores:\n",
    "                probs = torch.softmax(score, dim=-1)\n",
    "                max_prob = probs.max().item()\n",
    "                all_probs.append(max_prob)\n",
    "            words = transcription.strip().split()\n",
    "            if len(words) > 0 and len(all_probs) > 0:\n",
    "                avg_prob = np.mean(all_probs)\n",
    "                word_probs = [(word, avg_prob) for word in words]\n",
    "            else:\n",
    "                word_probs = [(word, 0.5) for word in words]\n",
    "        else:\n",
    "            words = transcription.strip().split()\n",
    "            word_probs = [(word, 0.8) for word in words]\n",
    "        \n",
    "        return word_probs\n",
    "    \n",
    "    def _extract_ctc_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n",
    "        inputs = self.processor(audio_array, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        input_values = inputs.input_values.to(self.device)\n",
    "        if self.use_fp16:\n",
    "            input_values = input_values.half()\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            logits = self.current_model(input_values).logits\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "        words = transcription.strip().split()\n",
    "        word_probs = []\n",
    "        \n",
    "        if len(words) > 0:\n",
    "            max_probs = probs.max(dim=-1).values.squeeze()\n",
    "            avg_confidence = max_probs.mean().item()\n",
    "            word_probs = [(word, avg_confidence) for word in words]\n",
    "        \n",
    "        return word_probs\n",
    "    \n",
    "    def _extract_seamless_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n",
    "        audio_inputs = self.processor(audios=audio_array, sampling_rate=16000, return_tensors=\"pt\").to(self.device)\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            output = self.current_model.generate(**audio_inputs, tgt_lang=\"urd\", return_dict_in_generate=True, output_scores=True)\n",
    "        \n",
    "        transcription = self.processor.decode(output.sequences[0].tolist(), skip_special_tokens=True)\n",
    "        word_probs = []\n",
    "        \n",
    "        if hasattr(output, 'scores') and output.scores:\n",
    "            all_probs = []\n",
    "            for score in output.scores:\n",
    "                probs = torch.softmax(score, dim=-1)\n",
    "                max_prob = probs.max().item()\n",
    "                all_probs.append(max_prob)\n",
    "            words = transcription.strip().split()\n",
    "            if len(words) > 0 and len(all_probs) > 0:\n",
    "                avg_prob = np.mean(all_probs)\n",
    "                word_probs = [(word, avg_prob) for word in words]\n",
    "            else:\n",
    "                word_probs = [(word, 0.7) for word in words]\n",
    "        else:\n",
    "            words = transcription.strip().split()\n",
    "            word_probs = [(word, 0.7) for word in words]\n",
    "        \n",
    "        return word_probs\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        if self.current_model is not None:\n",
    "            del self.current_model\n",
    "            self.current_model = None\n",
    "        if self.processor is not None:\n",
    "            del self.processor\n",
    "            self.processor = None\n",
    "        self.current_model_name = None\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    def clear_audio_cache(self):\n",
    "        self.audio_cache.clear()\n",
    "        gc.collect()\n",
    "    \n",
    "    def word_probabilities(self, audio_file_path: str, model_name: str) -> List[Tuple[str, float]]:\n",
    "        try:\n",
    "            audio_array = self._preprocess_audio(audio_file_path)\n",
    "            self._load_model(model_name)\n",
    "            \n",
    "            if \"whisper\" in model_name:\n",
    "                results = self._extract_whisper_probabilities(audio_array)\n",
    "            elif \"mms\" in model_name or \"wav2vec2\" in model_name:\n",
    "                results = self._extract_ctc_probabilities(audio_array)\n",
    "            elif \"seamless\" in model_name:\n",
    "                results = self._extract_seamless_probabilities(audio_array)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Error processing audio with {model_name}: {str(e)}\")\n",
    "\n",
    "def compute_wer(reference: str, hypothesis: str) -> float:\n",
    "    ref_words = reference.split()\n",
    "    hyp_words = hypothesis.split()\n",
    "    if len(ref_words) == 0:\n",
    "        return 0.0 if len(hyp_words) == 0 else 1.0\n",
    "    return editdistance.eval(ref_words, hyp_words) / len(ref_words)\n",
    "\n",
    "def compute_cer(reference: str, hypothesis: str) -> float:\n",
    "    if len(reference) == 0:\n",
    "        return 0.0 if len(hypothesis) == 0 else 1.0\n",
    "    return editdistance.eval(reference, hypothesis) / len(reference)\n",
    "\n",
    "def compute_ece(confidences: np.ndarray, accuracies: np.ndarray, n_bins: int = 10) -> float:\n",
    "    if len(confidences) == 0:\n",
    "        return 0.0\n",
    "    bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        bin_lower = bin_boundaries[i]\n",
    "        bin_upper = bin_boundaries[i + 1]\n",
    "        in_bin = (confidences > bin_lower) & (confidences <= bin_upper)\n",
    "        prop_in_bin = in_bin.mean()\n",
    "        if prop_in_bin > 0:\n",
    "            accuracy_in_bin = accuracies[in_bin].mean()\n",
    "            avg_confidence_in_bin = confidences[in_bin].mean()\n",
    "            ece += np.abs(avg_confidence_in_bin - accuracy_in_bin) * prop_in_bin\n",
    "    return ece\n",
    "\n",
    "CONFIG = {\n",
    "    'DATASET_PATH': \"/kaggle/input/coraldataset/ur\",\n",
    "    'MAX_SAMPLES': 10,\n",
    "    'OUTPUT_DIR': './iteration1_results',\n",
    "    'MODELS': [\n",
    "        \"whisper-small\",\n",
    "        \"whisper-medium\",\n",
    "        \"whisper-large\",\n",
    "        \"wav2vec2-urdu\",\n",
    "        \"mms-300m\",\n",
    "    ],\n",
    "    'DEVICE': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'USE_FP16': True,\n",
    "    'BATCH_CLEANUP': True\n",
    "}\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "def load_test_samples(dataset_path, max_samples):\n",
    "    dataset_path = Path(dataset_path)\n",
    "    tsv_file = dataset_path / \"other.tsv\"\n",
    "    \n",
    "    if not tsv_file.exists():\n",
    "        raise FileNotFoundError(f\"other.tsv not found at {tsv_file}\")\n",
    "    \n",
    "    samples = []\n",
    "    with open(tsv_file, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f, delimiter='\\t')\n",
    "        for i, row in enumerate(reader):\n",
    "            if i >= max_samples:\n",
    "                break\n",
    "            audio_path = dataset_path / \"clips\" / row['path']\n",
    "            if audio_path.exists():\n",
    "                samples.append({\n",
    "                    'audio_id': row['path'],\n",
    "                    'audio_path': str(audio_path),\n",
    "                    'reference': row['sentence'],\n",
    "                    'duration': 0.0\n",
    "                })\n",
    "    return samples\n",
    "\n",
    "def evaluate_model(asr_wrapper, model_name, test_samples):\n",
    "    results = []\n",
    "    try:\n",
    "        for sample in tqdm(test_samples, desc=model_name):\n",
    "            try:\n",
    "                word_probs = asr_wrapper.word_probabilities(sample['audio_path'], model_name)\n",
    "                hypothesis = ' '.join([w for w, p in word_probs])\n",
    "                reference = sample['reference']\n",
    "                wer = compute_wer(reference, hypothesis)\n",
    "                cer = compute_cer(reference, hypothesis)\n",
    "                avg_conf = np.mean([p for w, p in word_probs]) if word_probs else 0.0\n",
    "                ref_words = reference.split()\n",
    "                confidences = [p for w, p in word_probs]\n",
    "                accuracies = [1.0 if i < len(ref_words) and w == ref_words[i] else 0.0 \n",
    "                             for i, (w, p) in enumerate(word_probs)]\n",
    "                ece = compute_ece(np.array(confidences), np.array(accuracies)) if confidences else 0.0\n",
    "                results.append({\n",
    "                    'audio_id': sample['audio_id'],\n",
    "                    'model_name': model_name,\n",
    "                    'reference': reference,\n",
    "                    'hypothesis': hypothesis,\n",
    "                    'wer': wer,\n",
    "                    'cer': cer,\n",
    "                    'avg_confidence': avg_conf,\n",
    "                    'ece': ece,\n",
    "                    'duration': sample['duration']\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"\\nError on {sample['audio_id']}: {str(e)}\")\n",
    "                continue\n",
    "    finally:\n",
    "        asr_wrapper._cleanup()\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_plots(df, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    model_wer = df.groupby('model_name')['wer'].mean().sort_values()\n",
    "    plt.barh(model_wer.index, model_wer.values, color='steelblue')\n",
    "    plt.xlabel('Word Error Rate (WER)')\n",
    "    plt.title('Model Comparison: Average WER', fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'wer_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df.boxplot(column='wer', by='model_name')\n",
    "    plt.ylabel('WER')\n",
    "    plt.title('WER Distribution by Model', fontweight='bold')\n",
    "    plt.suptitle('')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'wer_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    model_ece = df.groupby('model_name')['ece'].mean().sort_values()\n",
    "    plt.barh(model_ece.index, model_ece.values, color='coral')\n",
    "    plt.xlabel('Expected Calibration Error (ECE)')\n",
    "    plt.title('Confidence Calibration by Model', fontweight='bold')\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_dir / 'calibration.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def run_iteration1():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CORAL ITERATION 1: BASELINE EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nTimestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    output_dir = Path(CONFIG['OUTPUT_DIR'])\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "    \n",
    "    print(f\"\\n[1/5] Loading test dataset from {CONFIG['DATASET_PATH']}...\")\n",
    "    test_samples = load_test_samples(CONFIG['DATASET_PATH'], CONFIG['MAX_SAMPLES'])\n",
    "    print(f\"Loaded {len(test_samples)} test samples\")\n",
    "    \n",
    "    print(f\"\\n[2/5] Initializing ASR wrapper on {CONFIG['DEVICE']}...\")\n",
    "    asr_wrapper = UrduASRWrapper(device=CONFIG['DEVICE'], use_fp16=CONFIG['USE_FP16'])\n",
    "    \n",
    "    print(f\"\\n[3/5] Evaluating {len(CONFIG['MODELS'])} models...\")\n",
    "    all_results = []\n",
    "    for model in CONFIG['MODELS']:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Model: {model}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        model_results = evaluate_model(asr_wrapper, model, test_samples)\n",
    "        all_results.extend(model_results)\n",
    "        if CONFIG['BATCH_CLEANUP']:\n",
    "            gc.collect()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "    \n",
    "    asr_wrapper.clear_audio_cache()\n",
    "    \n",
    "    df = pd.DataFrame(all_results)\n",
    "    \n",
    "    print(f\"\\n[4/5] Saving results...\")\n",
    "    df.to_csv(output_dir / 'detailed_results.csv', index=False, encoding='utf-8')\n",
    "    \n",
    "    aggregate = df.groupby('model_name').agg({\n",
    "        'wer': ['mean', 'std', 'min', 'max'],\n",
    "        'cer': ['mean', 'std'],\n",
    "        'avg_confidence': ['mean', 'std'],\n",
    "        'ece': ['mean', 'std'],\n",
    "        'duration': 'sum'\n",
    "    }).round(4)\n",
    "    aggregate.to_csv(output_dir / 'aggregate_metrics.csv')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"AGGREGATE METRICS\")\n",
    "    print(\"=\"*80)\n",
    "    print(aggregate)\n",
    "    \n",
    "    print(f\"\\n[5/5] Generating visualizations...\")\n",
    "    generate_plots(df, output_dir)\n",
    "    \n",
    "    report_file = output_dir / 'ITERATION1_REPORT.txt'\n",
    "    best_model = df.groupby('model_name')['wer'].mean().idxmin()\n",
    "    best_wer = df.groupby('model_name')['wer'].mean().min()\n",
    "    \n",
    "    with open(report_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\"*80 + \"\\n\")\n",
    "        f.write(\"CORAL PROJECT - ITERATION 1 EVALUATION REPORT\\n\")\n",
    "        f.write(\"=\"*80 + \"\\n\\n\")\n",
    "        f.write(f\"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "        f.write(f\"Samples: {len(test_samples)}\\n\")\n",
    "        f.write(f\"Models: {len(CONFIG['MODELS'])}\\n\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(\"BASELINE WER BY MODEL\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\\n\")\n",
    "        f.write(df.groupby('model_name')['wer'].describe().to_string())\n",
    "        f.write(\"\\n\\n\")\n",
    "        f.write(f\"BEST MODEL: {best_model}\\n\")\n",
    "        f.write(f\"BASELINE WER: {best_wer:.4f} ({best_wer*100:.2f}%)\\n\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        f.write(\"CALIBRATION ANALYSIS\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\\n\")\n",
    "        f.write(df.groupby('model_name')['ece'].describe().to_string())\n",
    "        f.write(\"\\n\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ITERATION 1 COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"\\nResults saved to: {output_dir.absolute()}\")\n",
    "    print(f\"Best Model: {best_model}\")\n",
    "    print(f\"Baseline WER: {best_wer*100:.2f}%\")\n",
    "    print(f\"Samples Evaluated: {len(df)}\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    return df, aggregate\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        results_df, aggregate_metrics = run_iteration1()\n",
    "        print(\"\\nEvaluation successful!\")\n",
    "        print(\"Review the results in ./iteration1_results/\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n\\nERROR: {str(e)}\")\n",
    "        raise\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coral-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
