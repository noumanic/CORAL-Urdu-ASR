{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-30T19:12:47.370087Z",
     "iopub.status.busy": "2025-09-30T19:12:47.369206Z",
     "iopub.status.idle": "2025-09-30T19:12:47.416406Z",
     "shell.execute_reply": "2025-09-30T19:12:47.415127Z",
     "shell.execute_reply.started": "2025-09-30T19:12:47.370049Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Iteration: 1 - CORAL-Urdu-ASR - CORAL_Iteration1_ASR_Ensemble.ipynb\n",
    "Urdu ASR Wrapper for Multiple Models\n",
    "Supports 8 diverse ASR models for Urdu speech recognition\n",
    "Optimized for Kaggle CPU/GPU notebooks with one-at-a-time loading\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Transformers imports\n",
    "from transformers import (\n",
    "    WhisperProcessor, \n",
    "    WhisperForConditionalGeneration,\n",
    "    Wav2Vec2Processor, \n",
    "    Wav2Vec2ForCTC,\n",
    "    SeamlessM4TForSpeechToText,\n",
    "    SeamlessM4TProcessor,\n",
    "    AutoProcessor,\n",
    "    AutoModelForCTC\n",
    ")\n",
    "\n",
    "\n",
    "class UrduASRWrapper:\n",
    "    \"\"\"\n",
    "    Unified wrapper for multiple Urdu ASR models.\n",
    "    Handles audio preprocessing, model loading, and word-probability extraction.\n",
    "    \"\"\"\n",
    "    \n",
    "    SUPPORTED_MODELS = {\n",
    "        \"whisper-large\": \"openai/whisper-large-v3\",\n",
    "        \"whisper-medium\": \"openai/whisper-medium\",\n",
    "        \"whisper-small\": \"openai/whisper-small\",\n",
    "        \"seamless-large\": \"facebook/seamless-m4t-v2-large\",\n",
    "        \"seamless-medium\": \"facebook/seamless-m4t-medium\",\n",
    "        \"mms-1b\": \"facebook/mms-1b-all\",\n",
    "        \"mms-300m\": \"facebook/mms-300m\",\n",
    "        \"wav2vec2-urdu\": \"kingabzpro/wav2vec2-large-xls-r-300m-Urdu\"\n",
    "    }\n",
    "    \n",
    "    def __init__(self, device: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the wrapper.\n",
    "        \n",
    "        Args:\n",
    "            device: 'cuda', 'cpu', or None (auto-detect)\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        else:\n",
    "            self.device = device\n",
    "        \n",
    "        print(f\"üöÄ ASR Wrapper initialized on: {self.device}\")\n",
    "        \n",
    "        self.current_model = None\n",
    "        self.processor = None\n",
    "        self.current_model_name = None\n",
    "    \n",
    "    def _preprocess_audio(self, file_path: str, target_sr: int = 16000) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert audio file to the required format.\n",
    "        Handles MP3, MP4, WAV, and other formats.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to audio file\n",
    "            target_sr: Target sample rate (default 16kHz)\n",
    "            \n",
    "        Returns:\n",
    "            Audio array (mono, 16kHz)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load audio with librosa (handles all formats)\n",
    "            audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n",
    "            \n",
    "            # Normalize audio to [-1, 1] range\n",
    "            if audio.dtype != np.float32:\n",
    "                audio = audio.astype(np.float32)\n",
    "            \n",
    "            # Normalize amplitude\n",
    "            max_val = np.abs(audio).max()\n",
    "            if max_val > 0:\n",
    "                audio = audio / max_val\n",
    "            \n",
    "            return audio\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error loading audio file {file_path}: {str(e)}\")\n",
    "    \n",
    "    def _load_model(self, model_name: str):\n",
    "        \"\"\"\n",
    "        Load a specific ASR model and its processor.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Key from SUPPORTED_MODELS\n",
    "        \"\"\"\n",
    "        if model_name not in self.SUPPORTED_MODELS:\n",
    "            raise ValueError(f\"Model {model_name} not supported. Choose from: {list(self.SUPPORTED_MODELS.keys())}\")\n",
    "        \n",
    "        model_id = self.SUPPORTED_MODELS[model_name]\n",
    "        print(f\"üì• Loading {model_name} ({model_id})...\")\n",
    "        \n",
    "        try:\n",
    "            # Load based on model family\n",
    "            if \"whisper\" in model_name:\n",
    "                self.processor = WhisperProcessor.from_pretrained(model_id)\n",
    "                self.current_model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "                \n",
    "            elif \"seamless\" in model_name:\n",
    "                self.processor = SeamlessM4TProcessor.from_pretrained(model_id)\n",
    "                self.current_model = SeamlessM4TForSpeechToText.from_pretrained(model_id)\n",
    "                \n",
    "            elif \"mms\" in model_name:\n",
    "                self.processor = AutoProcessor.from_pretrained(model_id)\n",
    "                self.current_model = AutoModelForCTC.from_pretrained(model_id)\n",
    "                \n",
    "            elif \"wav2vec2\" in model_name:\n",
    "                self.processor = Wav2Vec2Processor.from_pretrained(model_id)\n",
    "                self.current_model = Wav2Vec2ForCTC.from_pretrained(model_id)\n",
    "            \n",
    "            # Move to device\n",
    "            self.current_model = self.current_model.to(self.device)\n",
    "            self.current_model.eval()\n",
    "            self.current_model_name = model_name\n",
    "            \n",
    "            print(f\"‚úÖ {model_name} loaded successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to load model {model_name}: {str(e)}\")\n",
    "    \n",
    "    def _extract_whisper_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Extract word-probability pairs from Whisper models.\n",
    "        \n",
    "        Args:\n",
    "            audio_array: Preprocessed audio\n",
    "            \n",
    "        Returns:\n",
    "            List of (word, probability) tuples\n",
    "        \"\"\"\n",
    "        # Prepare input\n",
    "        input_features = self.processor(\n",
    "            audio_array, \n",
    "            sampling_rate=16000, \n",
    "            return_tensors=\"pt\"\n",
    "        ).input_features.to(self.device)\n",
    "        \n",
    "        # Generate with word timestamps\n",
    "        with torch.no_grad():\n",
    "            predicted_ids = self.current_model.generate(\n",
    "                input_features,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        # Decode transcription\n",
    "        transcription = self.processor.batch_decode(\n",
    "            predicted_ids.sequences, \n",
    "            skip_special_tokens=True\n",
    "        )[0]\n",
    "        \n",
    "        # Extract probabilities from scores\n",
    "        word_probs = []\n",
    "        if hasattr(predicted_ids, 'scores') and predicted_ids.scores:\n",
    "            # Get average probability across all tokens\n",
    "            all_probs = []\n",
    "            for score in predicted_ids.scores:\n",
    "                probs = torch.softmax(score, dim=-1)\n",
    "                max_prob = probs.max().item()\n",
    "                all_probs.append(max_prob)\n",
    "            \n",
    "            # Split transcription into words\n",
    "            words = transcription.strip().split()\n",
    "            \n",
    "            # Assign probabilities to words (distribute evenly)\n",
    "            if len(words) > 0 and len(all_probs) > 0:\n",
    "                avg_prob = np.mean(all_probs)\n",
    "                word_probs = [(word, avg_prob) for word in words]\n",
    "            else:\n",
    "                word_probs = [(word, 0.5) for word in words]\n",
    "        else:\n",
    "            # Fallback: assign default probability\n",
    "            words = transcription.strip().split()\n",
    "            word_probs = [(word, 0.8) for word in words]\n",
    "        \n",
    "        return word_probs\n",
    "    \n",
    "    def _extract_ctc_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Extract word-probability pairs from CTC models (MMS, Wav2Vec2).\n",
    "        \n",
    "        Args:\n",
    "            audio_array: Preprocessed audio\n",
    "            \n",
    "        Returns:\n",
    "            List of (word, probability) tuples\n",
    "        \"\"\"\n",
    "        # Prepare input\n",
    "        inputs = self.processor(\n",
    "            audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        input_values = inputs.input_values.to(self.device)\n",
    "        \n",
    "        # Get logits\n",
    "        with torch.no_grad():\n",
    "            logits = self.current_model(input_values).logits\n",
    "        \n",
    "        # Get probabilities\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Decode with CTC\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        # Extract word-level probabilities\n",
    "        words = transcription.strip().split()\n",
    "        word_probs = []\n",
    "        \n",
    "        if len(words) > 0:\n",
    "            # Calculate average confidence across the sequence\n",
    "            max_probs = probs.max(dim=-1).values.squeeze()\n",
    "            avg_confidence = max_probs.mean().item()\n",
    "            \n",
    "            # Assign to each word\n",
    "            word_probs = [(word, avg_confidence) for word in words]\n",
    "        \n",
    "        return word_probs\n",
    "    \n",
    "    def _extract_seamless_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Extract word-probability pairs from Seamless-M4T models.\n",
    "        \n",
    "        Args:\n",
    "            audio_array: Preprocessed audio\n",
    "            \n",
    "        Returns:\n",
    "            List of (word, probability) tuples\n",
    "        \"\"\"\n",
    "        # Prepare audio input\n",
    "        audio_inputs = self.processor(\n",
    "            audios=audio_array,\n",
    "            sampling_rate=16000,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        # Generate transcription\n",
    "        with torch.no_grad():\n",
    "            output = self.current_model.generate(\n",
    "                **audio_inputs,\n",
    "                tgt_lang=\"urd\",  # Urdu language code\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "        \n",
    "        # Decode transcription\n",
    "        transcription = self.processor.decode(\n",
    "            output.sequences[0].tolist(),\n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Extract probabilities\n",
    "        word_probs = []\n",
    "        if hasattr(output, 'scores') and output.scores:\n",
    "            all_probs = []\n",
    "            for score in output.scores:\n",
    "                probs = torch.softmax(score, dim=-1)\n",
    "                max_prob = probs.max().item()\n",
    "                all_probs.append(max_prob)\n",
    "            \n",
    "            words = transcription.strip().split()\n",
    "            if len(words) > 0 and len(all_probs) > 0:\n",
    "                avg_prob = np.mean(all_probs)\n",
    "                word_probs = [(word, avg_prob) for word in words]\n",
    "            else:\n",
    "                word_probs = [(word, 0.7) for word in words]\n",
    "        else:\n",
    "            words = transcription.strip().split()\n",
    "            word_probs = [(word, 0.7) for word in words]\n",
    "        \n",
    "        return word_probs\n",
    "    \n",
    "    def _cleanup(self):\n",
    "        \"\"\"Clean up memory after processing.\"\"\"\n",
    "        if self.current_model is not None:\n",
    "            del self.current_model\n",
    "            self.current_model = None\n",
    "        \n",
    "        if self.processor is not None:\n",
    "            del self.processor\n",
    "            self.processor = None\n",
    "        \n",
    "        self.current_model_name = None\n",
    "        \n",
    "        # Clear cache\n",
    "        if self.device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    def word_probabilities(\n",
    "        self, \n",
    "        audio_file_path: str, \n",
    "        model_name: str\n",
    "    ) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        Main function: Process audio and return word-probability pairs.\n",
    "        \n",
    "        Args:\n",
    "            audio_file_path: Path to audio file (MP3, MP4, WAV, etc.)\n",
    "            model_name: Model to use (key from SUPPORTED_MODELS)\n",
    "            \n",
    "        Returns:\n",
    "            List of (word, probability) tuples\n",
    "            Example: [(\"ÿ≥ŸÑÿßŸÖ\", 0.95), (\"ÿØŸÜ€åÿß\", 0.87), (\"ŸÖ€å⁄∫\", 0.92)]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"üéØ Processing: {Path(audio_file_path).name}\")\n",
    "            print(f\"ü§ñ Model: {model_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Step 1: Preprocess audio\n",
    "            print(\"üìä Preprocessing audio...\")\n",
    "            audio_array = self._preprocess_audio(audio_file_path)\n",
    "            print(f\"‚úÖ Audio loaded: {len(audio_array)/16000:.2f} seconds\")\n",
    "            \n",
    "            # Step 2: Load model\n",
    "            self._load_model(model_name)\n",
    "            \n",
    "            # Step 3: Extract probabilities based on model type\n",
    "            print(\"üîÑ Running inference...\")\n",
    "            \n",
    "            if \"whisper\" in model_name:\n",
    "                results = self._extract_whisper_probabilities(audio_array)\n",
    "            elif \"mms\" in model_name or \"wav2vec2\" in model_name:\n",
    "                results = self._extract_ctc_probabilities(audio_array)\n",
    "            elif \"seamless\" in model_name:\n",
    "                results = self._extract_seamless_probabilities(audio_array)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown model type: {model_name}\")\n",
    "            \n",
    "            print(f\"‚úÖ Transcription complete: {len(results)} words\")\n",
    "            print(f\"üìù Preview: {' '.join([w for w, p in results[:5]])}...\")\n",
    "            \n",
    "            # Step 4: Cleanup\n",
    "            self._cleanup()\n",
    "            print(\"üßπ Memory cleaned\")\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            self._cleanup()\n",
    "            raise RuntimeError(f\"Error processing audio with {model_name}: {str(e)}\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE FOR KAGGLE\n",
    "# ============================================================================\n",
    "\n",
    "def demo_usage():\n",
    "    \"\"\"Example usage for your FYP demo\"\"\"\n",
    "    \n",
    "    # Initialize wrapper\n",
    "    wrapper = UrduASRWrapper(device='cpu')  # Use 'cuda' if GPU available\n",
    "    \n",
    "    # Your audio file path\n",
    "    audio_path = \"test_urdu_audio.mp4\"\n",
    "    \n",
    "    # Process with all 8 models\n",
    "    models_to_test = [\n",
    "        \"whisper-large\",\n",
    "        \"whisper-medium\",\n",
    "        \"whisper-small\",\n",
    "        \"seamless-large\",\n",
    "        \"seamless-medium\",\n",
    "        \"mms-1b\",\n",
    "        \"mms-300m\",\n",
    "        \"wav2vec2-urdu\"\n",
    "    ]\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for model in models_to_test:\n",
    "        try:\n",
    "            results = wrapper.word_probabilities(audio_path, model)\n",
    "            all_results[model] = results\n",
    "            \n",
    "            # Display results\n",
    "            print(f\"\\n{model.upper()} Results:\")\n",
    "            print(f\"Transcription: {' '.join([w for w, p in results])}\")\n",
    "            print(f\"Avg Confidence: {np.mean([p for w, p in results]):.3f}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error with {model}: {str(e)}\")\n",
    "            all_results[model] = []\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "\n",
    "# Quick test function\n",
    "def test_single_model(audio_path: str, model_name: str = \"whisper-small\"):\n",
    "    \"\"\"Quick test with a single model\"\"\"\n",
    "    wrapper = UrduASRWrapper()\n",
    "    results = wrapper.word_probabilities(audio_path, model_name)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS:\")\n",
    "    print(\"=\"*60)\n",
    "    for word, prob in results:\n",
    "        print(f\"{word:20s} | Confidence: {prob:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:12:47.419382Z",
     "iopub.status.busy": "2025-09-30T19:12:47.418576Z",
     "iopub.status.idle": "2025-09-30T19:12:47.445705Z",
     "shell.execute_reply": "2025-09-30T19:12:47.444689Z",
     "shell.execute_reply.started": "2025-09-30T19:12:47.419315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example: Test with Mozilla Common Voice Urdu sample\n",
    "    print(\"Urdu ASR Wrapper - Ready for use!\")\n",
    "    print(f\"Supported models: {list(UrduASRWrapper.SUPPORTED_MODELS.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:12:47.447073Z",
     "iopub.status.busy": "2025-09-30T19:12:47.446719Z",
     "iopub.status.idle": "2025-09-30T19:12:47.477955Z",
     "shell.execute_reply": "2025-09-30T19:12:47.477105Z",
     "shell.execute_reply.started": "2025-09-30T19:12:47.447034Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "asr = UrduASRWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-30T19:12:47.480307Z",
     "iopub.status.busy": "2025-09-30T19:12:47.479096Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATASET_PATH = \"/kaggle/input/common-voice-ur/cv-corpus-22.0-delta-2025-06-20/ur\"\n",
    "probs = asr.word_probabilities(DATASET_PATH,\"whisper-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Add this cell after your ASR wrapper code in the Kaggle notebook\n",
    "\n",
    "# ============================================================================\n",
    "# FLASK APP WITH NGROK INTEGRATION + AUDIO RECORDING\n",
    "# ============================================================================\n",
    "\n",
    "from flask import Flask, render_template_string, request, jsonify, send_file\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "app = Flask(__name__)\n",
    "app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max\n",
    "\n",
    "# Create dataset directory\n",
    "DATASET_DIR = Path(\"/kaggle/working/recorded_dataset\")\n",
    "DATASET_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# HTML Template (Complete frontend with recording)\n",
    "HTML_TEMPLATE = '''\n",
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "    <title>Urdu Speech Recognition System</title>\n",
    "    <style>\n",
    "        * { margin: 0; padding: 0; box-sizing: border-box; }\n",
    "        body {\n",
    "            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            min-height: 100vh;\n",
    "            padding: 20px;\n",
    "        }\n",
    "        .container {\n",
    "            background: white;\n",
    "            border-radius: 20px;\n",
    "            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);\n",
    "            max-width: 1100px;\n",
    "            width: 100%;\n",
    "            margin: 0 auto;\n",
    "            padding: 40px;\n",
    "            animation: fadeIn 0.5s ease-in;\n",
    "        }\n",
    "        @keyframes fadeIn {\n",
    "            from { opacity: 0; transform: translateY(20px); }\n",
    "            to { opacity: 1; transform: translateY(0); }\n",
    "        }\n",
    "        .header {\n",
    "            text-align: center;\n",
    "            margin-bottom: 40px;\n",
    "        }\n",
    "        .header h1 {\n",
    "            color: #667eea;\n",
    "            font-size: 2.5em;\n",
    "            margin-bottom: 10px;\n",
    "            font-weight: 700;\n",
    "        }\n",
    "        .header p {\n",
    "            color: #666;\n",
    "            font-size: 1.1em;\n",
    "        }\n",
    "        .tabs {\n",
    "            display: flex;\n",
    "            gap: 10px;\n",
    "            margin-bottom: 30px;\n",
    "            border-bottom: 2px solid #e0e0e0;\n",
    "        }\n",
    "        .tab {\n",
    "            padding: 15px 30px;\n",
    "            background: none;\n",
    "            border: none;\n",
    "            cursor: pointer;\n",
    "            font-size: 1.1em;\n",
    "            font-weight: 600;\n",
    "            color: #666;\n",
    "            transition: all 0.3s ease;\n",
    "            border-bottom: 3px solid transparent;\n",
    "        }\n",
    "        .tab:hover {\n",
    "            color: #667eea;\n",
    "        }\n",
    "        .tab.active {\n",
    "            color: #667eea;\n",
    "            border-bottom-color: #667eea;\n",
    "        }\n",
    "        .tab-content {\n",
    "            display: none;\n",
    "        }\n",
    "        .tab-content.active {\n",
    "            display: block;\n",
    "            animation: fadeIn 0.3s ease-in;\n",
    "        }\n",
    "        .upload-section {\n",
    "            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n",
    "            border-radius: 15px;\n",
    "            padding: 40px;\n",
    "            text-align: center;\n",
    "            margin-bottom: 30px;\n",
    "            border: 2px dashed #667eea;\n",
    "            transition: all 0.3s ease;\n",
    "            cursor: pointer;\n",
    "        }\n",
    "        .upload-section:hover {\n",
    "            border-color: #764ba2;\n",
    "            transform: translateY(-2px);\n",
    "        }\n",
    "        .upload-icon { font-size: 4em; margin-bottom: 20px; }\n",
    "        .file-input { display: none; }\n",
    "        .file-label {\n",
    "            display: inline-block;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            padding: 15px 40px;\n",
    "            border-radius: 50px;\n",
    "            cursor: pointer;\n",
    "            font-size: 1.1em;\n",
    "            font-weight: 600;\n",
    "            transition: all 0.3s ease;\n",
    "        }\n",
    "        .file-label:hover {\n",
    "            transform: scale(1.05);\n",
    "            box-shadow: 0 5px 20px rgba(102, 126, 234, 0.4);\n",
    "        }\n",
    "        .file-name {\n",
    "            margin-top: 15px;\n",
    "            color: #667eea;\n",
    "            font-weight: 600;\n",
    "            font-size: 1.1em;\n",
    "        }\n",
    "        .record-section {\n",
    "            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n",
    "            border-radius: 15px;\n",
    "            padding: 40px;\n",
    "            text-align: center;\n",
    "            margin-bottom: 30px;\n",
    "        }\n",
    "        .record-button {\n",
    "            width: 120px;\n",
    "            height: 120px;\n",
    "            border-radius: 50%;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            border: none;\n",
    "            color: white;\n",
    "            font-size: 3em;\n",
    "            cursor: pointer;\n",
    "            transition: all 0.3s ease;\n",
    "            box-shadow: 0 5px 20px rgba(102, 126, 234, 0.3);\n",
    "            margin: 20px auto;\n",
    "            display: block;\n",
    "        }\n",
    "        .record-button:hover {\n",
    "            transform: scale(1.1);\n",
    "            box-shadow: 0 8px 30px rgba(102, 126, 234, 0.5);\n",
    "        }\n",
    "        .record-button.recording {\n",
    "            background: linear-gradient(135deg, #ff4444 0%, #cc0000 100%);\n",
    "            animation: pulse 1.5s infinite;\n",
    "        }\n",
    "        @keyframes pulse {\n",
    "            0%, 100% { transform: scale(1); }\n",
    "            50% { transform: scale(1.05); }\n",
    "        }\n",
    "        .record-timer {\n",
    "            font-size: 2em;\n",
    "            color: #667eea;\n",
    "            font-weight: 700;\n",
    "            margin: 20px 0;\n",
    "        }\n",
    "        .audio-player {\n",
    "            width: 100%;\n",
    "            margin: 20px 0;\n",
    "            display: none;\n",
    "        }\n",
    "        .audio-player.active {\n",
    "            display: block;\n",
    "        }\n",
    "        .save-recording-btn {\n",
    "            background: linear-gradient(135deg, #28a745 0%, #20c997 100%);\n",
    "            color: white;\n",
    "            border: none;\n",
    "            padding: 15px 40px;\n",
    "            border-radius: 50px;\n",
    "            font-size: 1.1em;\n",
    "            font-weight: 600;\n",
    "            cursor: pointer;\n",
    "            transition: all 0.3s ease;\n",
    "            margin: 10px;\n",
    "        }\n",
    "        .save-recording-btn:hover {\n",
    "            transform: scale(1.05);\n",
    "            box-shadow: 0 5px 20px rgba(40, 167, 69, 0.4);\n",
    "        }\n",
    "        .model-section { margin-bottom: 30px; }\n",
    "        .model-section h3 {\n",
    "            color: #333;\n",
    "            margin-bottom: 15px;\n",
    "            font-size: 1.3em;\n",
    "        }\n",
    "        .model-grid {\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(180px, 1fr));\n",
    "            gap: 15px;\n",
    "        }\n",
    "        .model-card {\n",
    "            background: white;\n",
    "            border: 2px solid #e0e0e0;\n",
    "            border-radius: 10px;\n",
    "            padding: 15px;\n",
    "            cursor: pointer;\n",
    "            transition: all 0.3s ease;\n",
    "            text-align: center;\n",
    "        }\n",
    "        .model-card:hover {\n",
    "            border-color: #667eea;\n",
    "            transform: translateY(-2px);\n",
    "            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.2);\n",
    "        }\n",
    "        .model-card.selected {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            border-color: #667eea;\n",
    "        }\n",
    "        .model-card input[type=\"radio\"] { display: none; }\n",
    "        .model-name { font-weight: 600; font-size: 1em; }\n",
    "        .model-desc { font-size: 0.85em; margin-top: 5px; opacity: 0.8; }\n",
    "        .process-btn {\n",
    "            width: 100%;\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            border: none;\n",
    "            padding: 18px;\n",
    "            border-radius: 50px;\n",
    "            font-size: 1.2em;\n",
    "            font-weight: 600;\n",
    "            cursor: pointer;\n",
    "            transition: all 0.3s ease;\n",
    "            margin-bottom: 20px;\n",
    "        }\n",
    "        .process-btn:hover:not(:disabled) {\n",
    "            transform: scale(1.02);\n",
    "            box-shadow: 0 10px 30px rgba(102, 126, 234, 0.4);\n",
    "        }\n",
    "        .process-btn:disabled { opacity: 0.6; cursor: not-allowed; }\n",
    "        .loading { display: none; text-align: center; padding: 30px; }\n",
    "        .loading.active { display: block; }\n",
    "        .spinner {\n",
    "            border: 4px solid #f3f3f3;\n",
    "            border-top: 4px solid #667eea;\n",
    "            border-radius: 50%;\n",
    "            width: 50px;\n",
    "            height: 50px;\n",
    "            animation: spin 1s linear infinite;\n",
    "            margin: 0 auto 20px;\n",
    "        }\n",
    "        @keyframes spin {\n",
    "            0% { transform: rotate(0deg); }\n",
    "            100% { transform: rotate(360deg); }\n",
    "        }\n",
    "        .results { display: none; margin-top: 30px; }\n",
    "        .results.active { display: block; animation: fadeIn 0.5s ease-in; }\n",
    "        .results h3 { color: #333; margin-bottom: 20px; font-size: 1.5em; }\n",
    "        .transcription-box {\n",
    "            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n",
    "            border-radius: 15px;\n",
    "            padding: 25px;\n",
    "            margin-bottom: 20px;\n",
    "            direction: rtl;\n",
    "            text-align: right;\n",
    "            font-size: 1.3em;\n",
    "            line-height: 1.8;\n",
    "            color: #333;\n",
    "            font-weight: 500;\n",
    "        }\n",
    "        .word-list {\n",
    "            background: #f9f9f9;\n",
    "            border-radius: 15px;\n",
    "            padding: 20px;\n",
    "            max-height: 400px;\n",
    "            overflow-y: auto;\n",
    "        }\n",
    "        .word-item {\n",
    "            background: white;\n",
    "            border-radius: 10px;\n",
    "            padding: 15px 20px;\n",
    "            margin-bottom: 10px;\n",
    "            display: flex;\n",
    "            justify-content: space-between;\n",
    "            align-items: center;\n",
    "            transition: all 0.3s ease;\n",
    "            border-left: 4px solid #667eea;\n",
    "        }\n",
    "        .word-item:hover {\n",
    "            transform: translateX(-5px);\n",
    "            box-shadow: 0 3px 10px rgba(0, 0, 0, 0.1);\n",
    "        }\n",
    "        .word-text {\n",
    "            font-size: 1.2em;\n",
    "            font-weight: 600;\n",
    "            color: #333;\n",
    "            direction: rtl;\n",
    "        }\n",
    "        .confidence { display: flex; align-items: center; gap: 10px; }\n",
    "        .confidence-bar {\n",
    "            width: 100px;\n",
    "            height: 8px;\n",
    "            background: #e0e0e0;\n",
    "            border-radius: 10px;\n",
    "            overflow: hidden;\n",
    "        }\n",
    "        .confidence-fill {\n",
    "            height: 100%;\n",
    "            background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);\n",
    "            transition: width 0.5s ease;\n",
    "        }\n",
    "        .confidence-text {\n",
    "            font-weight: 600;\n",
    "            color: #667eea;\n",
    "            min-width: 50px;\n",
    "        }\n",
    "        .stats {\n",
    "            display: grid;\n",
    "            grid-template-columns: repeat(auto-fit, minmax(150px, 1fr));\n",
    "            gap: 15px;\n",
    "            margin-top: 20px;\n",
    "        }\n",
    "        .stat-card {\n",
    "            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\n",
    "            color: white;\n",
    "            border-radius: 10px;\n",
    "            padding: 20px;\n",
    "            text-align: center;\n",
    "        }\n",
    "        .stat-value { font-size: 2em; font-weight: 700; margin-bottom: 5px; }\n",
    "        .stat-label { font-size: 0.9em; opacity: 0.9; }\n",
    "        .error, .success {\n",
    "            padding: 15px;\n",
    "            border-radius: 10px;\n",
    "            margin-top: 20px;\n",
    "            display: none;\n",
    "        }\n",
    "        .error {\n",
    "            background: #ff4444;\n",
    "            color: white;\n",
    "        }\n",
    "        .success {\n",
    "            background: #28a745;\n",
    "            color: white;\n",
    "        }\n",
    "        .error.active, .success.active { \n",
    "            display: block; \n",
    "            animation: fadeIn 0.5s ease-in; \n",
    "        }\n",
    "        .dataset-info {\n",
    "            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);\n",
    "            border-radius: 15px;\n",
    "            padding: 25px;\n",
    "            margin-top: 20px;\n",
    "            text-align: center;\n",
    "        }\n",
    "        .dataset-info h4 {\n",
    "            color: #667eea;\n",
    "            font-size: 1.3em;\n",
    "            margin-bottom: 15px;\n",
    "        }\n",
    "        .dataset-stat {\n",
    "            font-size: 1.5em;\n",
    "            font-weight: 700;\n",
    "            color: #333;\n",
    "            margin: 10px 0;\n",
    "        }\n",
    "        @media (max-width: 768px) {\n",
    "            .container { padding: 20px; }\n",
    "            .header h1 { font-size: 2em; }\n",
    "            .model-grid { grid-template-columns: 1fr; }\n",
    "            .tabs { overflow-x: auto; }\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <div class=\"header\">\n",
    "            <h1>üéôÔ∏è Urdu Speech Recognition</h1>\n",
    "            <p>Advanced AI-powered transcription & dataset collection system</p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"tabs\">\n",
    "            <button class=\"tab active\" onclick=\"switchTab('upload')\">üìÅ Upload Audio</button>\n",
    "            <button class=\"tab\" onclick=\"switchTab('record')\">üé§ Record Audio</button>\n",
    "        </div>\n",
    "\n",
    "        <!-- Upload Tab -->\n",
    "        <div class=\"tab-content active\" id=\"upload-tab\">\n",
    "            <div class=\"upload-section\" onclick=\"document.getElementById('audioFile').click()\">\n",
    "                <div class=\"upload-icon\">üìÅ</div>\n",
    "                <input type=\"file\" id=\"audioFile\" class=\"file-input\" accept=\"audio/*,video/*\">\n",
    "                <label for=\"audioFile\" class=\"file-label\">Choose Audio File</label>\n",
    "                <div class=\"file-name\" id=\"fileName\">No file selected</div>\n",
    "                <p style=\"margin-top: 15px; color: #666;\">Supports MP3, MP4, WAV, and more</p>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <!-- Record Tab -->\n",
    "        <div class=\"tab-content\" id=\"record-tab\">\n",
    "            <div class=\"record-section\">\n",
    "                <h3 style=\"color: #667eea; margin-bottom: 20px;\">üé§ Record Your Voice</h3>\n",
    "                <p style=\"color: #666; margin-bottom: 20px;\">Click the microphone to start recording</p>\n",
    "                \n",
    "                <button class=\"record-button\" id=\"recordBtn\" onclick=\"toggleRecording()\">üé§</button>\n",
    "                \n",
    "                <div class=\"record-timer\" id=\"recordTimer\">00:00</div>\n",
    "                \n",
    "                <audio class=\"audio-player\" id=\"audioPlayer\" controls></audio>\n",
    "                \n",
    "                <div id=\"recordActions\" style=\"display: none; margin-top: 20px;\">\n",
    "                    <button class=\"save-recording-btn\" onclick=\"saveToDataset()\">\n",
    "                        üíæ Save to Dataset\n",
    "                    </button>\n",
    "                    <button class=\"save-recording-btn\" style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);\" onclick=\"useForTranscription()\">\n",
    "                        üîÑ Use for Transcription\n",
    "                    </button>\n",
    "                </div>\n",
    "            </div>\n",
    "\n",
    "            <div class=\"dataset-info\">\n",
    "                <h4>üìä Dataset Statistics</h4>\n",
    "                <div class=\"dataset-stat\" id=\"datasetCount\">0 recordings saved</div>\n",
    "                <button class=\"file-label\" style=\"margin-top: 15px;\" onclick=\"downloadDataset()\">\n",
    "                    ‚¨áÔ∏è Download Dataset\n",
    "                </button>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"model-section\">\n",
    "            <h3>Select ASR Model</h3>\n",
    "            <div class=\"model-grid\">\n",
    "                <div class=\"model-card\" onclick=\"selectModel('whisper-small')\">\n",
    "                    <input type=\"radio\" name=\"model\" value=\"whisper-small\" id=\"whisper-small\">\n",
    "                    <div class=\"model-name\">Whisper Small</div>\n",
    "                    <div class=\"model-desc\">Fast</div>\n",
    "                </div>\n",
    "                <div class=\"model-card\" onclick=\"selectModel('whisper-medium')\">\n",
    "                    <input type=\"radio\" name=\"model\" value=\"whisper-medium\" id=\"whisper-medium\">\n",
    "                    <div class=\"model-name\">Whisper Medium</div>\n",
    "                    <div class=\"model-desc\">Balanced</div>\n",
    "                </div>\n",
    "                <div class=\"model-card\" onclick=\"selectModel('whisper-large')\">\n",
    "                    <input type=\"radio\" name=\"model\" value=\"whisper-large\" id=\"whisper-large\">\n",
    "                    <div class=\"model-name\">Whisper Large</div>\n",
    "                    <div class=\"model-desc\">Accurate</div>\n",
    "                </div>\n",
    "                <div class=\"model-card\" onclick=\"selectModel('wav2vec2-urdu')\">\n",
    "                    <input type=\"radio\" name=\"model\" value=\"wav2vec2-urdu\" id=\"wav2vec2-urdu\">\n",
    "                    <div class=\"model-name\">Wav2Vec2</div>\n",
    "                    <div class=\"model-desc\">Urdu</div>\n",
    "                </div>\n",
    "                <div class=\"model-card\" onclick=\"selectModel('mms-1b')\">\n",
    "                    <input type=\"radio\" name=\"model\" value=\"mms-1b\" id=\"mms-1b\">\n",
    "                    <div class=\"model-name\">MMS 1B</div>\n",
    "                    <div class=\"model-desc\">Multi</div>\n",
    "                </div>\n",
    "                <div class=\"model-card\" onclick=\"selectModel('seamless-medium')\">\n",
    "                    <input type=\"radio\" name=\"model\" value=\"seamless-medium\" id=\"seamless-medium\">\n",
    "                    <div class=\"model-name\">Seamless</div>\n",
    "                    <div class=\"model-desc\">Universal</div>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    "        <button class=\"process-btn\" onclick=\"processAudio()\" id=\"processBtn\">\n",
    "            üöÄ Start Transcription\n",
    "        </button>\n",
    "\n",
    "        <div class=\"loading\" id=\"loading\">\n",
    "            <div class=\"spinner\"></div>\n",
    "            <p style=\"color: #667eea; font-size: 1.1em; font-weight: 600;\">Processing your audio...</p>\n",
    "        </div>\n",
    "\n",
    "        <div class=\"error\" id=\"error\"></div>\n",
    "        <div class=\"success\" id=\"success\"></div>\n",
    "\n",
    "        <div class=\"results\" id=\"results\">\n",
    "            <h3>üìù Transcription Results</h3>\n",
    "            <div class=\"transcription-box\" id=\"transcription\"></div>\n",
    "            <div class=\"stats\">\n",
    "                <div class=\"stat-card\">\n",
    "                    <div class=\"stat-value\" id=\"wordCount\">0</div>\n",
    "                    <div class=\"stat-label\">Words</div>\n",
    "                </div>\n",
    "                <div class=\"stat-card\">\n",
    "                    <div class=\"stat-value\" id=\"avgConfidence\">0%</div>\n",
    "                    <div class=\"stat-label\">Avg Confidence</div>\n",
    "                </div>\n",
    "                <div class=\"stat-card\">\n",
    "                    <div class=\"stat-value\" id=\"duration\">0s</div>\n",
    "                    <div class=\"stat-label\">Duration</div>\n",
    "                </div>\n",
    "            </div>\n",
    "            <h3 style=\"margin-top: 30px;\">üìä Word-level Analysis</h3>\n",
    "            <div class=\"word-list\" id=\"wordList\"></div>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        let selectedModel = null;\n",
    "        let selectedFile = null;\n",
    "        let mediaRecorder = null;\n",
    "        let audioChunks = [];\n",
    "        let recordingStartTime = null;\n",
    "        let timerInterval = null;\n",
    "        let recordedBlob = null;\n",
    "\n",
    "        // Tab switching\n",
    "        function switchTab(tabName) {\n",
    "            document.querySelectorAll('.tab').forEach(tab => tab.classList.remove('active'));\n",
    "            document.querySelectorAll('.tab-content').forEach(content => content.classList.remove('active'));\n",
    "            \n",
    "            event.target.classList.add('active');\n",
    "            document.getElementById(tabName + '-tab').classList.add('active');\n",
    "        }\n",
    "\n",
    "        // File upload handling\n",
    "        document.getElementById('audioFile').addEventListener('change', function(e) {\n",
    "            if (e.target.files.length > 0) {\n",
    "                selectedFile = e.target.files[0];\n",
    "                document.getElementById('fileName').textContent = selectedFile.name;\n",
    "            }\n",
    "        });\n",
    "\n",
    "        // Recording functions\n",
    "        async function toggleRecording() {\n",
    "            if (mediaRecorder && mediaRecorder.state === 'recording') {\n",
    "                stopRecording();\n",
    "            } else {\n",
    "                startRecording();\n",
    "            }\n",
    "        }\n",
    "\n",
    "        async function startRecording() {\n",
    "            try {\n",
    "                const stream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
    "                mediaRecorder = new MediaRecorder(stream);\n",
    "                audioChunks = [];\n",
    "\n",
    "                mediaRecorder.ondataavailable = (event) => {\n",
    "                    audioChunks.push(event.data);\n",
    "                };\n",
    "\n",
    "                mediaRecorder.onstop = () => {\n",
    "                    recordedBlob = new Blob(audioChunks, { type: 'audio/mp3' });\n",
    "                    const audioUrl = URL.createObjectURL(recordedBlob);\n",
    "                    const audioPlayer = document.getElementById('audioPlayer');\n",
    "                    audioPlayer.src = audioUrl;\n",
    "                    audioPlayer.classList.add('active');\n",
    "                    document.getElementById('recordActions').style.display = 'block';\n",
    "                };\n",
    "\n",
    "                mediaRecorder.start();\n",
    "                document.getElementById('recordBtn').classList.add('recording');\n",
    "                document.getElementById('recordBtn').textContent = '‚èπÔ∏è';\n",
    "                \n",
    "                recordingStartTime = Date.now();\n",
    "                timerInterval = setInterval(updateTimer, 100);\n",
    "            } catch (err) {\n",
    "                showError('Microphone access denied: ' + err.message);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        function stopRecording() {\n",
    "            if (mediaRecorder && mediaRecorder.state === 'recording') {\n",
    "                mediaRecorder.stop();\n",
    "                mediaRecorder.stream.getTracks().forEach(track => track.stop());\n",
    "                document.getElementById('recordBtn').classList.remove('recording');\n",
    "                document.getElementById('recordBtn').textContent = 'üé§';\n",
    "                clearInterval(timerInterval);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        function updateTimer() {\n",
    "            const elapsed = Date.now() - recordingStartTime;\n",
    "            const seconds = Math.floor(elapsed / 1000);\n",
    "            const minutes = Math.floor(seconds / 60);\n",
    "            const secs = seconds % 60;\n",
    "            document.getElementById('recordTimer').textContent = \n",
    "                `${String(minutes).padStart(2, '0')}:${String(secs).padStart(2, '0')}`;\n",
    "        }\n",
    "\n",
    "        async function saveToDataset() {\n",
    "            if (!recordedBlob) {\n",
    "                showError('No recording to save');\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            const formData = new FormData();\n",
    "            formData.append('audio', recordedBlob, 'recording.mp3');\n",
    "\n",
    "            try {\n",
    "                const response = await fetch('/save_to_dataset', {\n",
    "                    method: 'POST',\n",
    "                    body: formData\n",
    "                });\n",
    "                const data = await response.json();\n",
    "                \n",
    "                if (data.success) {\n",
    "                    showSuccess('Recording saved to dataset! Total: ' + data.total_recordings);\n",
    "                    document.getElementById('datasetCount').textContent = \n",
    "                        data.total_recordings + ' recordings saved';\n",
    "                } else {\n",
    "                    showError(data.error);\n",
    "                }\n",
    "            } catch (error) {\n",
    "                showError('Error saving to dataset: ' + error.message);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        async function useForTranscription() {\n",
    "            if (!recordedBlob) {\n",
    "                showError('No recording available');\n",
    "                return;\n",
    "            }\n",
    "            selectedFile = new File([recordedBlob], 'recording.mp3', { type: 'audio/mp3' });\n",
    "            processAudio();\n",
    "        }\n",
    "\n",
    "        async function downloadDataset() {\n",
    "            try {\n",
    "                const response = await fetch('/download_dataset');\n",
    "                const blob = await response.blob();\n",
    "                const url = window.URL.createObjectURL(blob);\n",
    "                const a = document.createElement('a');\n",
    "                a.href = url;\n",
    "                a.download = 'urdu_dataset.zip';\n",
    "                document.body.appendChild(a);\n",
    "                a.click();\n",
    "                window.URL.revokeObjectURL(url);\n",
    "                document.body.removeChild(a);\n",
    "                showSuccess('Dataset downloaded successfully!');\n",
    "            } catch (error) {\n",
    "                showError('Error downloading dataset: ' + error.message);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Load dataset count on page load\n",
    "        async function loadDatasetStats() {\n",
    "            try {\n",
    "                const response = await fetch('/dataset_stats');\n",
    "                const data = await response.json();\n",
    "                document.getElementById('datasetCount').textContent = \n",
    "                    data.count + ' recordings saved';\n",
    "            } catch (error) {\n",
    "                console.error('Error loading dataset stats:', error);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // Model selection\n",
    "        function selectModel(modelName) {\n",
    "            selectedModel = modelName;\n",
    "            document.querySelectorAll('.model-card').forEach(card => {\n",
    "                card.classList.remove('selected');\n",
    "            });\n",
    "            document.querySelector(`#${modelName}`).closest('.model-card').classList.add('selected');\n",
    "        }\n",
    "\n",
    "        // Process audio\n",
    "        async function processAudio() {\n",
    "            if (!selectedFile) {\n",
    "                showError('Please select or record an audio file');\n",
    "                return;\n",
    "            }\n",
    "            if (!selectedModel) {\n",
    "                showError('Please select a model');\n",
    "                return;\n",
    "            }\n",
    "\n",
    "            const formData = new FormData();\n",
    "            formData.append('audio', selectedFile);\n",
    "            formData.append('model', selectedModel);\n",
    "\n",
    "            document.getElementById('processBtn').disabled = true;\n",
    "            document.getElementById('loading').classList.add('active');\n",
    "            document.getElementById('results').classList.remove('active');\n",
    "            document.getElementById('error').classList.remove('active');\n",
    "\n",
    "            try {\n",
    "                const response = await fetch('/transcribe', {\n",
    "                    method: 'POST',\n",
    "                    body: formData\n",
    "                });\n",
    "                const data = await response.json();\n",
    "\n",
    "                if (data.error) {\n",
    "                    showError(data.error);\n",
    "                } else {\n",
    "                    displayResults(data);\n",
    "                }\n",
    "            } catch (error) {\n",
    "                showError('Error processing audio: ' + error.message);\n",
    "            } finally {\n",
    "                document.getElementById('processBtn').disabled = false;\n",
    "                document.getElementById('loading').classList.remove('active');\n",
    "            }\n",
    "        }\n",
    "\n",
    "        function displayResults(data) {\n",
    "            const transcription = data.results.map(r => r.word).join(' ');\n",
    "            const avgConf = (data.results.reduce((sum, r) => sum + r.probability, 0) / data.results.length * 100).toFixed(1);\n",
    "\n",
    "            document.getElementById('transcription').textContent = transcription;\n",
    "            document.getElementById('wordCount').textContent = data.results.length;\n",
    "            document.getElementById('avgConfidence').textContent = avgConf + '%';\n",
    "            document.getElementById('duration').textContent = (data.audio_duration || 0).toFixed(1) + 's';\n",
    "\n",
    "            const wordListHtml = data.results.map(item => `\n",
    "                <div class=\"word-item\">\n",
    "                    <div class=\"word-text\">${item.word}</div>\n",
    "                    <div class=\"confidence\">\n",
    "                        <div class=\"confidence-bar\">\n",
    "                            <div class=\"confidence-fill\" style=\"width: ${item.probability * 100}%\"></div>\n",
    "                        </div>\n",
    "                        <div class=\"confidence-text\">${(item.probability * 100).toFixed(1)}%</div>\n",
    "                    </div>\n",
    "                </div>\n",
    "            `).join('');\n",
    "\n",
    "            document.getElementById('wordList').innerHTML = wordListHtml;\n",
    "            document.getElementById('results').classList.add('active');\n",
    "        }\n",
    "\n",
    "        function showError(message) {\n",
    "            const errorDiv = document.getElementById('error');\n",
    "            errorDiv.textContent = '‚ùå ' + message;\n",
    "            errorDiv.classList.add('active');\n",
    "            setTimeout(() => errorDiv.classList.remove('active'), 5000);\n",
    "        }\n",
    "\n",
    "        function showSuccess(message) {\n",
    "            const successDiv = document.getElementById('success');\n",
    "            successDiv.textContent = '‚úÖ ' + message;\n",
    "            successDiv.classList.add('active');\n",
    "            setTimeout(() => successDiv.classList.remove('active'), 5000);\n",
    "        }\n",
    "\n",
    "        // Load stats on page load\n",
    "        loadDatasetStats();\n",
    "    </script>\n",
    "</body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template_string(HTML_TEMPLATE)\n",
    "\n",
    "@app.route('/transcribe', methods=['POST'])\n",
    "def transcribe():\n",
    "    try:\n",
    "        if 'audio' not in request.files:\n",
    "            return jsonify({'error': 'No audio file provided'}), 400\n",
    "        \n",
    "        audio_file = request.files['audio']\n",
    "        model_name = request.form.get('model', 'whisper-small')\n",
    "        \n",
    "        if audio_file.filename == '':\n",
    "            return jsonify({'error': 'No file selected'}), 400\n",
    "        \n",
    "        # Save temporary file\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(audio_file.filename).suffix) as tmp_file:\n",
    "            audio_file.save(tmp_file.name)\n",
    "            tmp_path = tmp_file.name\n",
    "        \n",
    "        try:\n",
    "            # Get audio duration\n",
    "            import librosa\n",
    "            audio_data, sr = librosa.load(tmp_path, sr=16000)\n",
    "            duration = len(audio_data) / sr\n",
    "            \n",
    "            # Process with ASR\n",
    "            wrapper = UrduASRWrapper()\n",
    "            results = wrapper.word_probabilities(tmp_path, model_name)\n",
    "            \n",
    "            # Format results\n",
    "            formatted_results = [\n",
    "                {'word': word, 'probability': float(prob)}\n",
    "                for word, prob in results\n",
    "            ]\n",
    "            \n",
    "            return jsonify({\n",
    "                'success': True,\n",
    "                'results': formatted_results,\n",
    "                'model': model_name,\n",
    "                'audio_duration': duration\n",
    "            })\n",
    "            \n",
    "        finally:\n",
    "            if os.path.exists(tmp_path):\n",
    "                os.remove(tmp_path)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/save_to_dataset', methods=['POST'])\n",
    "def save_to_dataset():\n",
    "    try:\n",
    "        if 'audio' not in request.files:\n",
    "            return jsonify({'error': 'No audio file provided'}), 400\n",
    "        \n",
    "        audio_file = request.files['audio']\n",
    "        \n",
    "        # Generate unique filename with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')\n",
    "        filename = f\"urdu_audio_{timestamp}.mp3\"\n",
    "        file_path = DATASET_DIR / filename\n",
    "        \n",
    "        # Save the audio file\n",
    "        audio_file.save(str(file_path))\n",
    "        \n",
    "        # Create metadata file\n",
    "        metadata = {\n",
    "            'filename': filename,\n",
    "            'timestamp': timestamp,\n",
    "            'size': os.path.getsize(file_path),\n",
    "            'format': 'mp3'\n",
    "        }\n",
    "        \n",
    "        metadata_path = DATASET_DIR / f\"{filename}.json\"\n",
    "        with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        \n",
    "        # Count total recordings\n",
    "        total_recordings = len(list(DATASET_DIR.glob('*.mp3')))\n",
    "        \n",
    "        return jsonify({\n",
    "            'success': True,\n",
    "            'filename': filename,\n",
    "            'total_recordings': total_recordings,\n",
    "            'message': 'Recording saved successfully!'\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/dataset_stats', methods=['GET'])\n",
    "def dataset_stats():\n",
    "    try:\n",
    "        # Count MP3 files in dataset directory\n",
    "        mp3_files = list(DATASET_DIR.glob('*.mp3'))\n",
    "        \n",
    "        total_size = sum(f.stat().st_size for f in mp3_files)\n",
    "        total_size_mb = total_size / (1024 * 1024)\n",
    "        \n",
    "        return jsonify({\n",
    "            'count': len(mp3_files),\n",
    "            'total_size_mb': round(total_size_mb, 2),\n",
    "            'dataset_path': str(DATASET_DIR)\n",
    "        })\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/download_dataset', methods=['GET'])\n",
    "def download_dataset():\n",
    "    try:\n",
    "        import zipfile\n",
    "        import io\n",
    "        \n",
    "        # Create a zip file in memory\n",
    "        memory_file = io.BytesIO()\n",
    "        \n",
    "        with zipfile.ZipFile(memory_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            # Add all MP3 files\n",
    "            for mp3_file in DATASET_DIR.glob('*.mp3'):\n",
    "                zipf.write(mp3_file, mp3_file.name)\n",
    "            \n",
    "            # Add all metadata JSON files\n",
    "            for json_file in DATASET_DIR.glob('*.json'):\n",
    "                zipf.write(json_file, json_file.name)\n",
    "            \n",
    "            # Create a README\n",
    "            readme_content = f\"\"\"Urdu Audio Dataset\n",
    "=====================\n",
    "Total Recordings: {len(list(DATASET_DIR.glob('*.mp3')))}\n",
    "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "File Structure:\n",
    "- *.mp3: Audio recordings\n",
    "- *.json: Metadata for each recording\n",
    "\n",
    "This dataset was collected using the Urdu Speech Recognition System.\n",
    "\"\"\"\n",
    "            zipf.writestr('README.txt', readme_content)\n",
    "        \n",
    "        memory_file.seek(0)\n",
    "        \n",
    "        return send_file(\n",
    "            memory_file,\n",
    "            mimetype='application/zip',\n",
    "            as_attachment=True,\n",
    "            download_name=f'urdu_dataset_{datetime.now().strftime(\"%Y%m%d\")}.zip'\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "# Run Flask in background thread\n",
    "def run_flask():\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "# Start Flask server\n",
    "print(\"üöÄ Starting Flask server...\")\n",
    "flask_thread = Thread(target=run_flask, daemon=True)\n",
    "flask_thread.start()\n",
    "\n",
    "# Wait for Flask to start\n",
    "time.sleep(3)\n",
    "\n",
    "# Install and setup ngrok\n",
    "print(\"üì¶ Installing pyngrok...\")\n",
    "import sys\n",
    "!{sys.executable} -m pip install -q pyngrok\n",
    "\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Set your ngrok auth token\n",
    "print(\"üîê Setting up ngrok...\")\n",
    "ngrok.set_auth_token(\"2xZqpiP0G671IWuo5QikyhoWuYx_5CqC4z66tMzYaTcctUMzC\")\n",
    "\n",
    "# Create ngrok tunnel\n",
    "print(\"üåê Creating public URL...\")\n",
    "public_url = ngrok.connect(5000)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"‚úÖ SERVER IS LIVE!\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üåç Public URL: {public_url}\")\n",
    "print(f\"üìÅ Dataset Directory: {DATASET_DIR}\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüì± Features Available:\")\n",
    "print(\"  ‚Ä¢ Upload audio files for transcription\")\n",
    "print(\"  ‚Ä¢ Record audio directly from browser\")\n",
    "print(\"  ‚Ä¢ Save recordings to dataset\")\n",
    "print(\"  ‚Ä¢ Download complete dataset as ZIP\")\n",
    "print(\"  ‚Ä¢ 8 different ASR models supported\")\n",
    "print(\"\\n‚ö†Ô∏è  Keep this notebook running to maintain the connection\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8376858,
     "sourceId": 13216196,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "coral-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
