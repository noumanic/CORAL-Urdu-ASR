{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13216196,"sourceType":"datasetVersion","datasetId":8376858}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nUrdu ASR Wrapper for Multiple Models\nSupports 8 diverse ASR models for Urdu speech recognition\nOptimized for Kaggle CPU/GPU notebooks with one-at-a-time loading\n\"\"\"\n\nimport torch\nimport gc\nimport librosa\nimport soundfile as sf\nimport numpy as np\nfrom pathlib import Path\nfrom typing import List, Tuple, Dict\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Transformers imports\nfrom transformers import (\n    WhisperProcessor, \n    WhisperForConditionalGeneration,\n    Wav2Vec2Processor, \n    Wav2Vec2ForCTC,\n    SeamlessM4TForSpeechToText,\n    SeamlessM4TProcessor,\n    AutoProcessor,\n    AutoModelForCTC\n)\n\n\nclass UrduASRWrapper:\n    \"\"\"\n    Unified wrapper for multiple Urdu ASR models.\n    Handles audio preprocessing, model loading, and word-probability extraction.\n    \"\"\"\n    \n    SUPPORTED_MODELS = {\n        \"whisper-large\": \"openai/whisper-large-v3\",\n        \"whisper-medium\": \"openai/whisper-medium\",\n        \"whisper-small\": \"openai/whisper-small\",\n        \"seamless-large\": \"facebook/seamless-m4t-v2-large\",\n        \"seamless-medium\": \"facebook/seamless-m4t-medium\",\n        \"mms-1b\": \"facebook/mms-1b-all\",\n        \"mms-300m\": \"facebook/mms-300m\",\n        \"wav2vec2-urdu\": \"kingabzpro/wav2vec2-large-xls-r-300m-Urdu\"\n    }\n    \n    def __init__(self, device: str = None):\n        \"\"\"\n        Initialize the wrapper.\n        \n        Args:\n            device: 'cuda', 'cpu', or None (auto-detect)\n        \"\"\"\n        if device is None:\n            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        else:\n            self.device = device\n        \n        print(f\"üöÄ ASR Wrapper initialized on: {self.device}\")\n        \n        self.current_model = None\n        self.processor = None\n        self.current_model_name = None\n    \n    def _preprocess_audio(self, file_path: str, target_sr: int = 16000) -> np.ndarray:\n        \"\"\"\n        Convert audio file to the required format.\n        Handles MP3, MP4, WAV, and other formats.\n        \n        Args:\n            file_path: Path to audio file\n            target_sr: Target sample rate (default 16kHz)\n            \n        Returns:\n            Audio array (mono, 16kHz)\n        \"\"\"\n        try:\n            # Load audio with librosa (handles all formats)\n            audio, sr = librosa.load(file_path, sr=target_sr, mono=True)\n            \n            # Normalize audio to [-1, 1] range\n            if audio.dtype != np.float32:\n                audio = audio.astype(np.float32)\n            \n            # Normalize amplitude\n            max_val = np.abs(audio).max()\n            if max_val > 0:\n                audio = audio / max_val\n            \n            return audio\n            \n        except Exception as e:\n            raise ValueError(f\"Error loading audio file {file_path}: {str(e)}\")\n    \n    def _load_model(self, model_name: str):\n        \"\"\"\n        Load a specific ASR model and its processor.\n        \n        Args:\n            model_name: Key from SUPPORTED_MODELS\n        \"\"\"\n        if model_name not in self.SUPPORTED_MODELS:\n            raise ValueError(f\"Model {model_name} not supported. Choose from: {list(self.SUPPORTED_MODELS.keys())}\")\n        \n        model_id = self.SUPPORTED_MODELS[model_name]\n        print(f\"üì• Loading {model_name} ({model_id})...\")\n        \n        try:\n            # Load based on model family\n            if \"whisper\" in model_name:\n                self.processor = WhisperProcessor.from_pretrained(model_id)\n                self.current_model = WhisperForConditionalGeneration.from_pretrained(model_id)\n                \n            elif \"seamless\" in model_name:\n                self.processor = SeamlessM4TProcessor.from_pretrained(model_id)\n                self.current_model = SeamlessM4TForSpeechToText.from_pretrained(model_id)\n                \n            elif \"mms\" in model_name:\n                self.processor = AutoProcessor.from_pretrained(model_id)\n                self.current_model = AutoModelForCTC.from_pretrained(model_id)\n                \n            elif \"wav2vec2\" in model_name:\n                self.processor = Wav2Vec2Processor.from_pretrained(model_id)\n                self.current_model = Wav2Vec2ForCTC.from_pretrained(model_id)\n            \n            # Move to device\n            self.current_model = self.current_model.to(self.device)\n            self.current_model.eval()\n            self.current_model_name = model_name\n            \n            print(f\"‚úÖ {model_name} loaded successfully\")\n            \n        except Exception as e:\n            raise RuntimeError(f\"Failed to load model {model_name}: {str(e)}\")\n    \n    def _extract_whisper_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n        \"\"\"\n        Extract word-probability pairs from Whisper models.\n        \n        Args:\n            audio_array: Preprocessed audio\n            \n        Returns:\n            List of (word, probability) tuples\n        \"\"\"\n        # Prepare input\n        input_features = self.processor(\n            audio_array, \n            sampling_rate=16000, \n            return_tensors=\"pt\"\n        ).input_features.to(self.device)\n        \n        # Generate with word timestamps\n        with torch.no_grad():\n            predicted_ids = self.current_model.generate(\n                input_features,\n                return_dict_in_generate=True,\n                output_scores=True\n            )\n        \n        # Decode transcription\n        transcription = self.processor.batch_decode(\n            predicted_ids.sequences, \n            skip_special_tokens=True\n        )[0]\n        \n        # Extract probabilities from scores\n        word_probs = []\n        if hasattr(predicted_ids, 'scores') and predicted_ids.scores:\n            # Get average probability across all tokens\n            all_probs = []\n            for score in predicted_ids.scores:\n                probs = torch.softmax(score, dim=-1)\n                max_prob = probs.max().item()\n                all_probs.append(max_prob)\n            \n            # Split transcription into words\n            words = transcription.strip().split()\n            \n            # Assign probabilities to words (distribute evenly)\n            if len(words) > 0 and len(all_probs) > 0:\n                avg_prob = np.mean(all_probs)\n                word_probs = [(word, avg_prob) for word in words]\n            else:\n                word_probs = [(word, 0.5) for word in words]\n        else:\n            # Fallback: assign default probability\n            words = transcription.strip().split()\n            word_probs = [(word, 0.8) for word in words]\n        \n        return word_probs\n    \n    def _extract_ctc_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n        \"\"\"\n        Extract word-probability pairs from CTC models (MMS, Wav2Vec2).\n        \n        Args:\n            audio_array: Preprocessed audio\n            \n        Returns:\n            List of (word, probability) tuples\n        \"\"\"\n        # Prepare input\n        inputs = self.processor(\n            audio_array,\n            sampling_rate=16000,\n            return_tensors=\"pt\",\n            padding=True\n        )\n        \n        input_values = inputs.input_values.to(self.device)\n        \n        # Get logits\n        with torch.no_grad():\n            logits = self.current_model(input_values).logits\n        \n        # Get probabilities\n        probs = torch.softmax(logits, dim=-1)\n        \n        # Decode with CTC\n        predicted_ids = torch.argmax(logits, dim=-1)\n        transcription = self.processor.batch_decode(predicted_ids)[0]\n        \n        # Extract word-level probabilities\n        words = transcription.strip().split()\n        word_probs = []\n        \n        if len(words) > 0:\n            # Calculate average confidence across the sequence\n            max_probs = probs.max(dim=-1).values.squeeze()\n            avg_confidence = max_probs.mean().item()\n            \n            # Assign to each word\n            word_probs = [(word, avg_confidence) for word in words]\n        \n        return word_probs\n    \n    def _extract_seamless_probabilities(self, audio_array: np.ndarray) -> List[Tuple[str, float]]:\n        \"\"\"\n        Extract word-probability pairs from Seamless-M4T models.\n        \n        Args:\n            audio_array: Preprocessed audio\n            \n        Returns:\n            List of (word, probability) tuples\n        \"\"\"\n        # Prepare audio input\n        audio_inputs = self.processor(\n            audios=audio_array,\n            sampling_rate=16000,\n            return_tensors=\"pt\"\n        ).to(self.device)\n        \n        # Generate transcription\n        with torch.no_grad():\n            output = self.current_model.generate(\n                **audio_inputs,\n                tgt_lang=\"urd\",  # Urdu language code\n                return_dict_in_generate=True,\n                output_scores=True\n            )\n        \n        # Decode transcription\n        transcription = self.processor.decode(\n            output.sequences[0].tolist(),\n            skip_special_tokens=True\n        )\n        \n        # Extract probabilities\n        word_probs = []\n        if hasattr(output, 'scores') and output.scores:\n            all_probs = []\n            for score in output.scores:\n                probs = torch.softmax(score, dim=-1)\n                max_prob = probs.max().item()\n                all_probs.append(max_prob)\n            \n            words = transcription.strip().split()\n            if len(words) > 0 and len(all_probs) > 0:\n                avg_prob = np.mean(all_probs)\n                word_probs = [(word, avg_prob) for word in words]\n            else:\n                word_probs = [(word, 0.7) for word in words]\n        else:\n            words = transcription.strip().split()\n            word_probs = [(word, 0.7) for word in words]\n        \n        return word_probs\n    \n    def _cleanup(self):\n        \"\"\"Clean up memory after processing.\"\"\"\n        if self.current_model is not None:\n            del self.current_model\n            self.current_model = None\n        \n        if self.processor is not None:\n            del self.processor\n            self.processor = None\n        \n        self.current_model_name = None\n        \n        # Clear cache\n        if self.device == \"cuda\":\n            torch.cuda.empty_cache()\n        gc.collect()\n    \n    def word_probabilities(\n        self, \n        audio_file_path: str, \n        model_name: str\n    ) -> List[Tuple[str, float]]:\n        \"\"\"\n        Main function: Process audio and return word-probability pairs.\n        \n        Args:\n            audio_file_path: Path to audio file (MP3, MP4, WAV, etc.)\n            model_name: Model to use (key from SUPPORTED_MODELS)\n            \n        Returns:\n            List of (word, probability) tuples\n            Example: [(\"ÿ≥ŸÑÿßŸÖ\", 0.95), (\"ÿØŸÜ€åÿß\", 0.87), (\"ŸÖ€å⁄∫\", 0.92)]\n        \"\"\"\n        try:\n            print(f\"\\n{'='*60}\")\n            print(f\"üéØ Processing: {Path(audio_file_path).name}\")\n            print(f\"ü§ñ Model: {model_name}\")\n            print(f\"{'='*60}\")\n            \n            # Step 1: Preprocess audio\n            print(\"üìä Preprocessing audio...\")\n            audio_array = self._preprocess_audio(audio_file_path)\n            print(f\"‚úÖ Audio loaded: {len(audio_array)/16000:.2f} seconds\")\n            \n            # Step 2: Load model\n            self._load_model(model_name)\n            \n            # Step 3: Extract probabilities based on model type\n            print(\"üîÑ Running inference...\")\n            \n            if \"whisper\" in model_name:\n                results = self._extract_whisper_probabilities(audio_array)\n            elif \"mms\" in model_name or \"wav2vec2\" in model_name:\n                results = self._extract_ctc_probabilities(audio_array)\n            elif \"seamless\" in model_name:\n                results = self._extract_seamless_probabilities(audio_array)\n            else:\n                raise ValueError(f\"Unknown model type: {model_name}\")\n            \n            print(f\"‚úÖ Transcription complete: {len(results)} words\")\n            print(f\"üìù Preview: {' '.join([w for w, p in results[:5]])}...\")\n            \n            # Step 4: Cleanup\n            self._cleanup()\n            print(\"üßπ Memory cleaned\")\n            \n            return results\n            \n        except Exception as e:\n            self._cleanup()\n            raise RuntimeError(f\"Error processing audio with {model_name}: {str(e)}\")\n\n\n# ============================================================================\n# USAGE EXAMPLE FOR KAGGLE\n# ============================================================================\n\ndef demo_usage():\n    \"\"\"Example usage for your FYP demo\"\"\"\n    \n    # Initialize wrapper\n    wrapper = UrduASRWrapper(device='cpu')  # Use 'cuda' if GPU available\n    \n    # Your audio file path\n    audio_path = \"test_urdu_audio.mp4\"\n    \n    # Process with all 8 models\n    models_to_test = [\n        \"whisper-large\",\n        \"whisper-medium\",\n        \"whisper-small\",\n        \"seamless-large\",\n        \"seamless-medium\",\n        \"mms-1b\",\n        \"mms-300m\",\n        \"wav2vec2-urdu\"\n    ]\n    \n    all_results = {}\n    \n    for model in models_to_test:\n        try:\n            results = wrapper.word_probabilities(audio_path, model)\n            all_results[model] = results\n            \n            # Display results\n            print(f\"\\n{model.upper()} Results:\")\n            print(f\"Transcription: {' '.join([w for w, p in results])}\")\n            print(f\"Avg Confidence: {np.mean([p for w, p in results]):.3f}\")\n            \n        except Exception as e:\n            print(f\"‚ùå Error with {model}: {str(e)}\")\n            all_results[model] = []\n    \n    return all_results\n\n\n# Quick test function\ndef test_single_model(audio_path: str, model_name: str = \"whisper-small\"):\n    \"\"\"Quick test with a single model\"\"\"\n    wrapper = UrduASRWrapper()\n    results = wrapper.word_probabilities(audio_path, model_name)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"RESULTS:\")\n    print(\"=\"*60)\n    for word, prob in results:\n        print(f\"{word:20s} | Confidence: {prob:.3f}\")\n    \n    return results","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # Example: Test with Mozilla Common Voice Urdu sample\n    print(\"Urdu ASR Wrapper - Ready for use!\")\n    print(f\"Supported models: {list(UrduASRWrapper.SUPPORTED_MODELS.keys())}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"asr = UrduASRWrapper()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = \"/kaggle/input/urdudataset/15026046341 15026046337/cv-corpus-22.0-delta-2025-06-20/ur/clips/common_voice_ur_42810146.mp3\"\nprobs = asr.word_probabilities(file_path,\"whisper-large\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(probs)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}