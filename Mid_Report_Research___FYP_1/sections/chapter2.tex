% sections/chapter2.tex
\chapter{Literature Review}
\label{sec:literature}

This chapter critically examines existing literature relevant to the CORAL project. We analyze key research in multi-ASR fusion, confidence estimation, LLM-based error correction, and low-resource language ASR systems, establishing the theoretical and empirical foundation for our approach.

\section{Related Research}

We review five seminal papers that directly inform the CORAL architecture's design and methodology.

\subsection{Multi-ASR Fusion with LLM-Based Post-Editing}

\subsubsection{Summary: Prakash et al. (2025)}

Prakash et al. \cite{prakash2025} introduce a novel approach for generating high-quality pseudo-labels for semi-supervised ASR training by unifying outputs from multiple end-to-end ASR models using LLM-based post-editing. Their method integrates three diverse ASR systems: Icefall, Nemo Parakeet, and Whisper, leveraging their complementary strengths.

The key innovation is their two-stage approach:

\begin{enumerate}
    \item \textbf{Text-Based LLM Post-Editing}: An instruction-tuned LLM processes textual hypotheses from all three ASR systems, using natural language prompts to correct errors and synthesize a final transcript.
    
    \item \textbf{SpeechLLM with Audio Input}: A multimodal LLM that processes both textual hypotheses and the original audio signal, enabling audio-informed correction decisions.
\end{enumerate}

On LibriSpeech datasets, their approach achieved approximately 14\% relative Word Error Rate Reduction (WERR) after fusion. When used to generate pseudo-labels for semi-supervised training, ASR models retrained on these pseudo-labels achieved 3.22\% WER on LibriSpeech test-clean versus 3.40\% for the baseline, approaching near-human-level performance.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}
    \item Demonstrates the effectiveness of LLM-based multi-ASR fusion
    \item Achieves significant relative error reduction (14\% WERR)
    \item Novel integration of audio signals into LLM correction process
    \item Validated on widely-recognized benchmark datasets
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Requires expensive LLM fine-tuning, limiting accessibility
    \item High computational requirements (GPU-intensive)
    \item Tested exclusively on high-resource English datasets
    \item No evaluation on low-resource languages or code-switched speech
    \item Complexity of multimodal SpeechLLM limits practical deployment
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL builds directly upon this work but addresses its limitations for low-resource scenarios:

\begin{itemize}
    \item \textbf{No Fine-Tuning Required}: CORAL uses black-box instruction-tuned LLMs with zero-shot prompting, eliminating fine-tuning costs
    \item \textbf{Confidence-Guided Fusion}: Instead of only textual hypotheses, CORAL provides explicit word-level confidence scores to guide LLM decisions
    \item \textbf{Low-Resource Focus}: Specifically designed and evaluated for Urdu, a genuinely low-resource language
    \item \textbf{Text-Only Simplicity}: Maintains practicality by not requiring audio input to LLM, reducing complexity and computational costs
\end{itemize}

\subsection{Novel Confidence Estimation for ASR}

\subsubsection{Summary: Nagarathna et al. (2025)}

Nagarathna et al. \cite{nagarathna2025} propose TruCLeS (True Class Lexical Similarity Score), a novel continuous confidence metric for ASR systems that combines model probability scores with lexical similarity measures.

Traditional confidence metrics rely solely on model output probabilities, which often suffer from miscalibration. TruCLeS addresses this by:

\begin{enumerate}
    \item Computing ASR model probability scores for predicted tokens
    \item Calculating lexical similarity between predictions and potential ground-truth transcripts
    \item Combining both signals into a unified continuous confidence score
\end{enumerate}

The method requires ground-truth transcripts for training a supervised confidence model but demonstrates superior calibration performance across multiple metrics (Mean Absolute Error, Kullback-Leibler Divergence, Jensen-Shannon Divergence) compared to binary confidence baselines.

On in-domain Hindi data, TruCLeS reduced MAE from 0.108 to 0.087, showing consistent improvements in confidence calibration quality.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}
    \item Novel approach combining probability and lexical similarity
    \item Demonstrated improvement in calibration metrics
    \item Applicable to low-resource languages (tested on Hindi)
    \item Addresses fundamental miscalibration issues in neural ASR
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Requires ground-truth transcripts for training, limiting scalability
    \item Adds computational overhead of auxiliary confidence model
    \item Improves calibration but does not directly reduce WER
    \item Complexity may limit real-time deployment scenarios
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL takes a complementary approach to confidence estimation:

\begin{itemize}
    \item \textbf{Zero-Shot Confidence}: CORAL extracts raw model confidences without requiring additional training or ground-truth data
    \item \textbf{Ensemble Diversity}: Instead of improving individual model calibration, CORAL leverages confidence differences across multiple models
    \item \textbf{Practical Deployment}: Avoids auxiliary models, maintaining computational efficiency
    \item \textbf{Direct WER Impact}: Uses confidence scores for hypothesis selection and fusion, directly impacting final transcription accuracy
\end{itemize}

While TruCLeS focuses on improving confidence quality, CORAL focuses on leveraging diverse confidence signals for hypothesis correction.

\subsection{Domain-Specific LLM-Based ASR Correction}

\subsubsection{Summary: Koilakuntla et al. (2024)}

Koilakuntla et al. \cite{koilakuntla2024} present a targeted approach for correcting specific ASR errors in contact center environments using GPT-3.5 with retrieval-augmented generation. Their method addresses the challenge of domain-specific terminology (brand names, product codes, technical terms) that generic ASR systems frequently misrecognize.

The approach uses:

\begin{enumerate}
    \item \textbf{Error Pattern Identification}: Analyzes transcripts to identify specific error types (e.g., brand name misrecognitions)
    \item \textbf{Retrieval-Augmented Correction}: Uses context anchors and domain knowledge bases to provide GPT-3.5 with relevant correction candidates
    \item \textbf{Targeted Post-Processing}: Applies corrections selectively to identified error patterns rather than reprocessing entire transcripts
\end{enumerate}

The system corrected 3,201 instances of specific errors compared to 3,050 manual corrections, completing the task in 0.08 hours versus 15 hours manually - a dramatic efficiency improvement.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}
    \item Model-agnostic: Works with any external ASR provider
    \item Highly effective for targeted error correction
    \item Dramatic reduction in manual correction time (99.5\% reduction)
    \item Practical deployment in production environments
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Highly specialized for specific domains (contact centers)
    \item Limited generalizability to other error types or domains
    \item Requires careful prompt engineering for each error category
    \item Needs curated retrieval databases for domain-specific terms
    \item No overall WER improvement reported - only targeted correction metrics
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL differs fundamentally in scope and approach:

\begin{itemize}
    \item \textbf{General-Purpose Correction}: CORAL aims for overall WER reduction across all error types, not just specific patterns
    \item \textbf{Open-Domain Application}: Designed for diverse Urdu speech domains without requiring domain-specific engineering
    \item \textbf{Confidence-Driven}: Uses confidence scores from ensemble to guide corrections, rather than pattern matching
    \item \textbf{No Retrieval Required}: Operates without external knowledge bases, leveraging LLM's intrinsic linguistic knowledge
\end{itemize}

However, CORAL can learn from their success in structured prompt design for LLM-based correction.

\subsection{Hybrid-E2E ASR Ensemble for Low-Resource Languages}

\subsubsection{Summary: Parikh et al. (2024)}

Parikh et al. \cite{parikh2024} address ASR for Irish, a genuinely low-resource language, by combining complementary strengths of hybrid HMM-Kaldi systems and end-to-end Wav2Vec2.0 models through calibrated ROVER (Recognizer Output Voting Error Reduction) fusion.

Their approach involves:

\begin{enumerate}
    \item \textbf{Diverse System Architectures}: Combining traditional hybrid HMM-DNN (Kaldi) with modern self-supervised E2E (Wav2Vec2.0 XLS-R)
    \item \textbf{Confidence Calibration}: Addressing overconfidence issues in E2E models using Renyi's entropy-based calibration with temperature scaling
    \item \textbf{ROVER Fusion}: Word-level weighted voting based on calibrated confidence scores
\end{enumerate}

On Irish test data, their tuned ROVER ensemble achieved 22.94\% WER, representing a 14\% relative improvement over the best single model (25.81\% WER). The work demonstrates that even simple fusion techniques can yield significant gains for low-resource scenarios.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}
    \item Demonstrates effectiveness for genuinely low-resource language (Irish)
    \item Achieves 14-20\% relative WER reduction through ensemble
    \item Addresses E2E model overconfidence through principled calibration
    \item Combines traditional and modern ASR approaches effectively
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Uses traditional ROVER fusion without modern LLM capabilities
    \item Requires building and maintaining two distinct ASR systems (hybrid + E2E)
    \item Absolute WER remains relatively high (22.94\%)
    \item Confidence calibration requires tuning on development set
    \item No mechanism for linguistic coherence beyond voting
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL extends this ensemble concept with modern techniques:

\begin{itemize}
    \item \textbf{LLM-Based Fusion}: Replaces ROVER voting with instruction-guided LLM reasoning for linguistically coherent correction
    \item \textbf{Zero-Shot Calibration}: Avoids explicit calibration tuning by leveraging multiple models' raw confidences
    \item \textbf{Multiple E2E Models}: Uses diverse pre-trained E2E models (Whisper variants, Wav2Vec2) without requiring hybrid systems
    \item \textbf{Similar Target}: Like Irish, Urdu is a low-resource language, making this work highly relevant
\end{itemize}

CORAL aims to achieve similar or better relative gains with lower system complexity.

\subsection{Code-Mixed ASR for Urdu-English}

\subsubsection{Summary: Naqvi \& Tahir (2024)}

Naqvi and Tahir \cite{naqvi2024} develop a specialized hybrid ASR system for Urdu-English code-mixed street addresses in navigation contexts. They address the specific challenge of code-switching between Urdu (in Perso-Arabic script) and English (in Roman script) within the narrow domain of spoken addresses.

Their approach involves:

\begin{enumerate}
    \item \textbf{Specialized Corpus Collection}: 61.8 hours of general Urdu speech and 16.9 hours of Roman-Urdu/English addresses
    \item \textbf{Hybrid Architecture}: Kaldi-based system with TDNN-LSTM acoustic models
    \item \textbf{Custom Lexicon}: Combining Unicode Urdu and Romanized transcripts for code-mixed handling
    \item \textbf{Domain Optimization}: Deep specialization for navigation/address domain
\end{enumerate}

The system achieved remarkably low WER (4.02\%) and CER (0.8\%) on code-mixed street addresses, representing a 70-80\% absolute WER reduction compared to initial baselines.

\subsubsection{Critical Analysis}

\textbf{Strengths:}
\begin{itemize}
    \item Extremely low error rates in target domain (4.02\% WER)
    \item Explicitly handles Urdu-English code-switching
    \item Practical deployment for navigation applications
    \item Demonstrates feasibility of Urdu ASR with sufficient domain data
\end{itemize}

\textbf{Weaknesses:}
\begin{itemize}
    \item Very narrow scope: limited to street addresses and navigation
    \item Not an ensemble or LLM-based approach
    \item Requires extensive domain-specific data collection and engineering
    \item Single hybrid system without multi-model benefits
    \item Unlikely to generalize to other Urdu domains or open-domain speech
\end{itemize}

\subsubsection{Relationship to CORAL}

CORAL complements this work by targeting broader applicability:

\begin{itemize}
    \item \textbf{Open-Domain Focus}: CORAL aims for general Urdu ASR without domain restrictions
    \item \textbf{Code-Switching Capability}: Multilingual pre-trained models (Whisper, Wav2Vec2-XLSR) inherently handle code-switching
    \item \textbf{No Custom Data Required}: Leverages pre-trained models without domain-specific corpus collection
    \item \textbf{Ensemble Benefits}: Multiple models provide robustness across various code-switching patterns
\end{itemize}

While Naqvi \& Tahir achieve superior performance in their narrow domain, CORAL pursues broader applicability with acceptable performance tradeoffs.

\section{Analysis Summary}

Table \ref{tab:literature_summary} provides a comprehensive comparison of the reviewed research papers across key dimensions relevant to CORAL.

\begin{table}[H]
\centering
\caption{Comparative Analysis of Related Research}
\label{tab:literature_summary}
\resizebox{\textwidth}{!}{
\begin{tabular}{@{}lp{3cm}p{3cm}p{3cm}p{3cm}@{}}
\toprule
\textbf{Study} & \textbf{Key Strengths} & \textbf{Limitations} & \textbf{Best Results} & \textbf{CORAL's Innovation} \\ \midrule

Prakash et al. (2025) & 
Multi-ASR fusion with LLM; Audio-aware SpeechLLM; 14\% WERR & 
Requires LLM fine-tuning; High compute; English-only & 
3.22\% WER on LibriSpeech test-clean & 
Black-box LLM; Confidence-guided; Urdu-focused \\ \midrule

Nagarathna et al. (2025) & 
Novel confidence metric (TruCLeS); Better calibration & 
Requires ground truth; No WER improvement & 
MAE reduced from 0.108 to 0.087 & 
Zero-shot confidence; Ensemble diversity; Direct WER impact \\ \midrule

Koilakuntla et al. (2024) & 
Domain-specific correction; 99.5\% time reduction & 
Narrow scope; Requires custom prompts & 
3,201 corrections in 0.08 hours & 
General-purpose; Open-domain; No retrieval needed \\ \midrule

Parikh et al. (2024) & 
Low-resource (Irish); 14\% relative improvement; Hybrid+E2E & 
Traditional ROVER; Needs calibration tuning & 
22.94\% WER on Irish & 
LLM-based fusion; Zero-shot calibration; E2E-only \\ \midrule

Naqvi \& Tahir (2024) & 
Code-mixed Urdu-English; 4.02\% WER in domain & 
Very narrow (addresses only); Single system & 
4.02\% WER, 0.8\% CER & 
Open-domain; Ensemble; No custom data \\ \bottomrule
\end{tabular}
}
\end{table}

\subsection{Research Gaps Addressed by CORAL}

Based on the literature analysis, we identify five critical gaps that CORAL addresses:

\begin{enumerate}
    \item \textbf{Multi-ASR Fusion for Low-Resource Languages}: While Prakash et al. demonstrate LLM-based fusion effectiveness, their work is limited to high-resource English. CORAL extends this approach specifically to low-resource Urdu without requiring expensive fine-tuning.
    
    \item \textbf{Practical Confidence Utilization}: Nagarathna et al. improve confidence calibration but require supervised training. CORAL leverages raw confidence scores in a zero-shot manner, making them immediately actionable for hypothesis correction.
    
    \item \textbf{General-Purpose Open-Domain Correction}: Koilakuntla et al. excel in narrow domains with specific error patterns. CORAL provides general-purpose correction across all error types without domain-specific engineering.
    
    \item \textbf{Modern Fusion Beyond ROVER}: Parikh et al. demonstrate ensemble benefits for low-resource Irish but use traditional ROVER voting. CORAL employs instruction-tuned LLMs for linguistically coherent fusion that considers context beyond simple voting.
    
    \item \textbf{Scalable Code-Mixing Handling}: Naqvi \& Tahir achieve excellent results in navigation domain but require extensive domain-specific data collection. CORAL leverages multilingual pre-trained models' inherent code-switching capabilities for broader applicability.
\end{enumerate}

\subsection{CORAL's Unique Contributions}

The CORAL framework makes several unique contributions to the field:

\begin{itemize}
    \item \textbf{First Confidence-Guided LLM Fusion for Low-Resource ASR}: Combines explicit word-level confidence scores with black-box LLM reasoning for Urdu ASR correction.
    
    \item \textbf{Zero-Shot Multi-Model Correction}: Operates entirely with pre-trained models without requiring fine-tuning, calibration tuning, or supervised confidence training.
    
    \item \textbf{Practical Deployment Architecture}: Balances performance and computational efficiency through text-only LLM processing and efficient model ensemble management.
    
    \item \textbf{Comprehensive Evaluation Framework}: Establishes baseline metrics (WER, CER, confidence, ECE) for Urdu ASR ensemble systems.
\end{itemize}

\section{Theoretical Foundation}

The CORAL architecture is grounded in three key theoretical principles:

\subsection{Ensemble Learning Theory}

Ensemble methods achieve superior performance by combining predictions from multiple diverse models. The effectiveness depends on:

\begin{enumerate}
    \item \textbf{Model Diversity}: Different models make different errors due to architectural differences, training data, and optimization objectives.
    
    \item \textbf{Complementary Strengths}: Each model excels in different acoustic conditions, linguistic contexts, or phonetic patterns.
    
    \item \textbf{Error Reduction}: If models are uncorrelated, ensemble combination can reduce overall error rate.
\end{enumerate}

CORAL leverages diversity across:
\begin{itemize}
    \item Architecture types (encoder-decoder vs. CTC)
    \item Model sizes (244M to 1.5B parameters)
    \item Training paradigms (supervised multilingual vs. self-supervised + fine-tuning)
\end{itemize}

\subsection{Confidence-Based Decision Making}

Uncertainty quantification is crucial for reliable ASR systems. Confidence scores provide:

\begin{enumerate}
    \item \textbf{Uncertainty Estimates}: Explicit indication of model confidence in predictions
    \item \textbf{Error Detection}: Low confidence correlates with higher error likelihood
    \item \textbf{Selective Correction}: High-confidence predictions can be trusted; low-confidence predictions should be scrutinized
\end{enumerate}

CORAL extracts and utilizes confidence scores to:
\begin{itemize}
    \item Guide LLM's hypothesis selection
    \item Weight multiple predictions
    \item Identify ambiguous regions requiring careful reasoning
\end{itemize}

\subsection{Large Language Model Reasoning}

Modern instruction-tuned LLMs possess:

\begin{enumerate}
    \item \textbf{Linguistic Knowledge}: Deep understanding of grammar, semantics, and context
    \item \textbf{Instruction Following}: Ability to perform tasks specified in natural language prompts
    \item \textbf{Reasoning Capabilities}: Can weigh evidence, resolve ambiguities, and make coherent decisions
\end{enumerate}

CORAL leverages LLM capabilities to:
\begin{itemize}
    \item Synthesize multiple hypotheses into linguistically coherent output
    \item Consider confidence scores alongside linguistic plausibility
    \item Maintain contextual consistency across transcription
\end{itemize}

\section{Summary}

This literature review establishes that while significant progress has been made in ASR error correction through LLM-based post-editing, confidence estimation, and ensemble methods, critical gaps remain for low-resource languages like Urdu. Existing approaches either require expensive fine-tuning (Prakash et al.), supervised training (Nagarathna et al.), domain-specific engineering (Koilakuntla et al., Naqvi \& Tahir), or lack modern LLM-based reasoning (Parikh et al.).

CORAL uniquely addresses these gaps by combining confidence-guided multi-model fusion with black-box instruction-tuned LLMs, specifically targeting Urdu ASR without requiring fine-tuning, calibration tuning, or domain-specific data collection. The theoretical foundations in ensemble learning, confidence-based decision making, and LLM reasoning provide a solid basis for the proposed architecture.

Iteration 1's successful implementation of the multi-model hypothesis generation pipeline with confidence extraction validates the feasibility of the first stage of this approach, setting the foundation for the instruction-guided correction mechanism to be implemented in Iteration 2.